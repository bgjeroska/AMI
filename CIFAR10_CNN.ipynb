{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Convolutional Neural Networks\n",
    "\n",
    "In this notebook, you will work again on the CIFAR-10 classification task. This time a standard approach to the image classification  - namely Convolutional Neural Networks - will be applied. [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a classic image recognition problem, consisting of 60,000 32x32 pixel RGB images (50,000 for training and 10,000 for testing) in 10 categories: plane, car, bird, cat, deer, dog, frog, horse, ship, truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.utils.layer_utils import count_params\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import *\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://storage.googleapis.com/kaggle-competitions/kaggle/3649/media/cifar-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined to plotting functions for you which can be used in this (and other) notebooks. The first one plots a confustion matrix. The second one plots the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_true, y_pred, classlabels=None,ax=None, xlabel='predicted class',ylabel='actual class',cmtitle='Confusion Matrix'):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    conf = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(conf, annot=True, fmt=\"d\",cmap=sns.color_palette(\"Blues\"),ax=ax)\n",
    "    if classlabels is not None:\n",
    "        ax.set(xticklabels=classlabels)\n",
    "        ax.set(yticklabels=classlabels)\n",
    "    ax.set(xlabel=xlabel)\n",
    "    ax.set(ylabel=ylabel)\n",
    "    ax.set(title=cmtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_history(history, ax=None, metric='loss', ep_start=1, ep_stop=None,monitor='val_loss', mode='min',plttitle=None):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    if ep_stop is None:\n",
    "        ep_stop = len(history.epoch)\n",
    "    if plttitle is None:\n",
    "        plttitle = metric[0].swapcase() + metric[1:] + ' During Training'\n",
    "    ax.plot(np.arange(ep_start,ep_stop+1, dtype='int'),history.history[metric][ep_start-1:ep_stop])\n",
    "    ax.plot(np.arange(ep_start,ep_stop+1, dtype='int'),history.history['val_' + metric][ep_start-1:ep_stop])\n",
    "    ax.set(title=plttitle)\n",
    "    ax.set(ylabel=metric[0].swapcase() + metric[1:])\n",
    "    ax.set(xlabel='Epoch')\n",
    "    ax.legend(['train', 'val'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same dataset as in the last notebook and - due to computation time - you are again free to choose which fraction of the data you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "classlabels = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog','horse','boat','truck']\n",
    "#only take a fourth of the dataset, i.e. the first fold\n",
    "dataset_split = 4\n",
    "skf = StratifiedKFold(n_splits=dataset_split,shuffle=True,random_state=42)\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    X = X[test_index]\n",
    "    y = y[test_index]\n",
    "    break\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the images to floats between 0 and 1\n",
    "X_train, X_test = X_train.astype('float32')/255, X_test.astype('float32')/255\n",
    "# convert the labels to one-hot encoding\n",
    "(y_train, y_test)=  np_utils.to_categorical(y_train,10), np_utils.to_categorical(y_test,10)\n",
    "# mean center the images\n",
    "X_train_mean = np.expand_dims(X_train.mean(axis=0),axis=0)\n",
    "X_train = X_train-X_train_mean\n",
    "X_test = X_test-X_train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs use similar/same concepts as the convolution you might know from your signal processing or computer vision courses. When dealing with a 2D array we could convolve an grey scale image with a 2D filter. In most cases though we are working on colored images. In CNNs this fact is handeled by using a filter kernel for each channel. So an image is of size $H_I$x$W_I$x$3$ and the kernel of size $H_k$x$W_k$x$3$. After 3 2D-convolutions the result is finally summed up, such that the output is of dimension $H_O$x$W_O$x$1$. The spatial dimensions depend on the padding you apply. In CNN a bias would be added subsequently to the 3 convolutions and summation and finally the output would be passed through an activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "kernel = np.array([[-1, -2, -1],[0,0,0],[1,2,1]])\n",
    "X_train_conv = np.empty(X_train.shape[:-1])\n",
    "X_test_conv = np.empty(X_test.shape[:-1])\n",
    "for k in range(len(X_train)): \n",
    "    X_train_conv[k,:,:] = ndimage.convolve(X_train[k,:,:,0], kernel, mode='constant', cval=0.0)+ndimage.convolve(X_train[k,:,:,1], kernel, mode='constant', cval=0.0)+ndimage.convolve(X_train[k,:,:,2], kernel, mode='constant', cval=0.0)\n",
    "for k in range(len(X_test)): \n",
    "    X_test_conv[k,:,:] = ndimage.convolve(X_test[k,:,:,0], kernel, mode='constant', cval=0.0)+ndimage.convolve(X_test[k,:,:,1], kernel, mode='constant', cval=0.0)+ndimage.convolve(X_test[k,:,:,2], kernel, mode='constant', cval=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(X_train[10]+np.squeeze(X_train_mean))\n",
    "ax[0].grid(False)\n",
    "ax[1].imshow(X_train_conv[10])\n",
    "ax[1].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets do a short experiment. Check if the filtered images can be used in feed forward network. We used a sobel filter in each channel. In contrast a CNN would learn its filter coefficients, let us check if this works better.\n",
    "\n",
    "_Remark:_ For the `Conv2D()` layer below, we use the number of filters to be 1, `padding='same'`, `activation=None` and `use_bias=False`. This ensures a fair comparison. In the summary you can see, that the network will have  to learn 27 parameters more than one with the sobel input. This is 3 times 3 for each kernel times 3 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_sobel():\n",
    "    nn = Sequential()\n",
    "    nn.add(Flatten(input_shape=X_train_conv.shape[1:]))\n",
    "    nn.add(Dense(512,activation='relu',name=\"hidden1\"))\n",
    "    nn.add(Dense(64,activation='relu',name=\"hidden2\"))\n",
    "    nn.add(Dense(10, activation='softmax'))\n",
    "    #Compile mlp\n",
    "    nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return nn\n",
    "def build_model_learn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Conv2D(1,(3,3),activation=None,input_shape=X_train.shape[1:],padding='same', use_bias=False))\n",
    "    nn.add(Flatten())\n",
    "    nn.add(Dense(512,activation='relu',name=\"hidden1\"))\n",
    "    nn.add(Dense(64,activation='relu',name=\"hidden2\"))\n",
    "    nn.add(Dense(10, activation='softmax'))\n",
    "    #Compile mlp\n",
    "    nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_sobel()\n",
    "model.summary()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8, restore_best_weights=True)\n",
    "history = model.fit(X_train_conv, y_train, validation_split=0.2, \n",
    "                    epochs=50, batch_size=64, callbacks=[es], verbose=0)\n",
    "model.evaluate(X_test_conv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_learn()\n",
    "model.summary()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, \n",
    "                    epochs=50, batch_size=64, callbacks=[es], verbose=0)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, after warm up we now want to build a CNN to classify the CIFAR images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the last notebook on feed forward networks we write a `build_model()` function that returns a Keras sequential model - now suited for a 10-class classification problem. We use an alternation of 2D convolutional layers and pooling layers (keeping it at 3 and 2 layers of each). A filtersize of 5 and 3 and a number of filters of 32 and 64 could be a good trade off for non-gpu machines in a teaching scenario. Add the parameter `name='conv1'`,`name='conv2'` and `name='conv3'` to the `Conv2D()` layers - we will use that later on.\n",
    "On top, add a flatten layer and connect one dense hidden layer and the dense output layer. Check your model using `model.summary()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(32,(5,5), activation='relu', input_shape=(32, 32, 3),name='conv1'))\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', name='conv2'))\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', name='conv3'))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation='relu'))\n",
    "    cnn.add(Dense(10, activation='softmax'))\n",
    "    #Compile mlp\n",
    "    cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return cnn\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset with early stopping (using a sensible validation split). Next step is plotting the accuracy and the loss during the training and a confusion matrix on test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=64, callbacks=[es], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(8,3))\n",
    "plot_model_history(history, ax=ax[0])\n",
    "plot_model_history(history, metric='accuracy',ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,5))\n",
    "plot_cm(y_train.argmax(axis=1),y_train_pred.argmax(axis=1), ax=ax[0],classlabels=classlabels,cmtitle='Confusion Matrix (train)')\n",
    "plot_cm(y_train.argmax(axis=1),y_train_pred.argmax(axis=1), ax=ax[1],classlabels=classlabels, cmtitle='Confusion Matrix (test)')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the trainable parameters of your model and build a feedforward with 2 hidden layers and a similar amount of parameters. Play around with the size factor, if you found a model with a similar number of parameters, train it and compare the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trainable parameters CNN: ' + str(count_params(model.trainable_weights)))\n",
    "def build_model_mlp(size_fac=1):\n",
    "    mlp = Sequential()\n",
    "    mlp.add(Flatten(input_shape=(X_train.shape[1:])))\n",
    "    mlp.add(Dense(int(size_fac*3),activation='relu',name=\"hidden1\"))\n",
    "    mlp.add(Dense(int(size_fac*2), activation='relu',name=\"hidden2\"))\n",
    "    mlp.add(Dense(10, activation='softmax',name=\"output\"))\n",
    "    mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return mlp\n",
    "mlp = build_model_mlp(21)\n",
    "print('Trainable parameters MLP: ' + str(count_params(mlp.trainable_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(32,(5,5), activation='relu', input_shape=(32, 32, 3),name='conv1'))\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', name='conv2'))\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', name='conv3'))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation='relu'))\n",
    "    cnn.add(Dense(10, activation='softmax'))\n",
    "    #Compile mlp\n",
    "    cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return cnn\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8, restore_best_weights=True)\n",
    "history = mlp.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=64, callbacks=[es], verbose=0)\n",
    "print('CNN', model.evaluate(X_test,y_test))\n",
    "print('MLP',mlp.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get an idea what a CNN learns? To get an intuation we will do two things: First, we will inspect some activations in different layers. Therefore we just interpret the output of our convolutional layers as images. \n",
    "    A more sophisticated visualization can be done by learning which images maximize the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'conv1' ## change the layer\n",
    "\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "layer_output = layer_dict[layer_name].output\n",
    "activation_model = Model(inputs=model.input, outputs=layer_output) \n",
    "activations = activation_model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picnb = 100  # change the picture in the training set\n",
    "f = plt.figure(figsize=[2,2])\n",
    "plt.imshow(X_test[picnb,:,:,:] + np.squeeze(X_train_mean))\n",
    "plt.grid(False)\n",
    "f,axs = plt.subplots(4,8,figsize=[10,5])\n",
    "for fnb,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.imshow(activations[picnb,:,:,fnb])\n",
    "    ax.grid(False)\n",
    "plt.suptitle('First 32 activations of layer ' + layer_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we inspected the activations of the CNN layers for natural images from our test set. Even more interesting might be how an image has to look like to maximize the output of a filter. This is only slightly more complcated than plotting the activations. Basically we start with a black image and change the pixel intensities in a way to maximize the output of a filter. This is called \"gradient ascent\". The functions below will do the job for you. It is taken (and only slightly adopted) from the [Keras Documentation](https://keras.io/examples/conv_filter_visualization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from PIL import Image as pil_image\n",
    "from tensorflow.python.keras.preprocessing.image import save_img\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"utility function to normalize a tensor.\n",
    "\n",
    "    # Arguments\n",
    "        x: An input tensor.\n",
    "\n",
    "    # Returns\n",
    "        The normalized input tensor.\n",
    "    \"\"\"\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array representing the generated image.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array, which could be used in e.g. imshow.\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(x, former):\n",
    "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
    "       Reverses `deprocess_image`.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array, which could be used in e.g. imshow.\n",
    "        former: The former numpy-array.\n",
    "                Need to determine the former mean and variance.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array representing the generated image.\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer(model,\n",
    "                    layer_name,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(412, 412),\n",
    "                    filter_range=(0, None)):\n",
    "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
    "\n",
    "    # Arguments\n",
    "        model: The model containing layer_name.\n",
    "        layer_name: The name of the layer to be visualized.\n",
    "                    Has to be a part of model.\n",
    "        step: step size for gradient ascent.\n",
    "        epochs: Number of iterations for gradient ascent.\n",
    "        upscaling_steps: Number of upscaling steps.\n",
    "                         Starting image is in this case (80, 80).\n",
    "        upscaling_factor: Factor to which to slowly upgrade\n",
    "                          the image towards output_dim.\n",
    "        output_dim: [img_width, img_height] The output image dimensions.\n",
    "        filter_range: Tupel[lower, upper]\n",
    "                      Determines the to be computed filter numbers.\n",
    "                      If the second value is `None`,\n",
    "                      the last filter will be inferred as the upper boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_filter_image(input_img,\n",
    "                               layer_output,\n",
    "                               filter_index):\n",
    "        \"\"\"Generates image for one particular filter.\n",
    "\n",
    "        # Arguments\n",
    "            input_img: The input-image Tensor.\n",
    "            layer_output: The output-image Tensor.\n",
    "            filter_index: The to be processed filter number.\n",
    "                          Assumed to be valid.\n",
    "\n",
    "        #Returns\n",
    "            Either None if no image could be generated.\n",
    "            or a tuple of the image (array) itself and the last loss.\n",
    "        \"\"\"\n",
    "        s_time = time.time()\n",
    "\n",
    "        # we build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "    \n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "        #grads = K.gradients\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        # we start from a gray image with some random noise\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.random.random(\n",
    "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
    "        else:\n",
    "            input_img_data = np.random.random(\n",
    "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
    "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "        # Slowly upscaling towards the original size prevents\n",
    "        # a dominating high-frequency of the to visualized structure\n",
    "        # as it would occur if we directly compute the 412d-image.\n",
    "        # Behaves as a better starting point for each following dimension\n",
    "        # and therefore avoids poor local minima\n",
    "        for up in reversed(range(upscaling_steps)):\n",
    "            # we run gradient ascent for e.g. 20 steps\n",
    "            for _ in range(epochs):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data += grads_value * step\n",
    "\n",
    "                # some filters get stuck to 0, we can skip them\n",
    "                if loss_value <= K.epsilon():\n",
    "                    return None\n",
    "\n",
    "            # Calculate upscaled dimension\n",
    "            intermediate_dim = tuple(\n",
    "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "            # Upscale\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                           pil_image.BICUBIC))\n",
    "            input_img_data = np.expand_dims(\n",
    "                process_image(img, input_img_data[0]), 0)\n",
    "\n",
    "        # decode the resulting input image\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        e_time = time.time()\n",
    "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                                  loss_value,\n",
    "                                                                  e_time - s_time))\n",
    "        return img, loss_value\n",
    "\n",
    "    def _draw_filters(filters, n=None):\n",
    "        \"\"\"Draw the best filters in a nxn grid.\n",
    "\n",
    "        # Arguments\n",
    "            filters: A List of generated images and their corresponding losses\n",
    "                     for each processed filter.\n",
    "            n: dimension of the grid.\n",
    "               If none, the largest possible square will be used\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "        # the filters that have the highest loss are assumed to be better-looking.\n",
    "        # we will only keep the top n*n filters.\n",
    "        filters.sort(key=lambda x: x[1], reverse=True)\n",
    "        filters = filters[:n * n]\n",
    "\n",
    "        # build a black picture with enough space for\n",
    "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
    "        MARGIN = 5\n",
    "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "        # fill the picture with our saved filters\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                img, _ = filters[i * n + j]\n",
    "                width_margin = (output_dim[0] + MARGIN) * i\n",
    "                height_margin = (output_dim[1] + MARGIN) * j\n",
    "                stitched_filters[\n",
    "                    width_margin: width_margin + output_dim[0],\n",
    "                    height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "        # save the result to disk\n",
    "        save_img('cnn_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "\n",
    "    # this is the placeholder for the input images\n",
    "    assert len(model.inputs) == 1\n",
    "    input_img = model.inputs[0]\n",
    "\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[0:]])\n",
    "\n",
    "    output_layer = layer_dict[layer_name]\n",
    "    assert isinstance(output_layer, layers.Conv2D)\n",
    "\n",
    "    # Compute to be processed filter range\n",
    "    filter_lower = filter_range[0]\n",
    "    filter_upper = (filter_range[1]\n",
    "                    if filter_range[1] is not None\n",
    "                    else len(output_layer.get_weights()[1]))\n",
    "    assert(filter_lower >= 0\n",
    "           and filter_upper <= len(output_layer.get_weights()[1])\n",
    "           and filter_upper > filter_lower)\n",
    "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
    "\n",
    "    # iterate through each filter and generate its corresponding image\n",
    "    processed_filters = []\n",
    "    for f in range(filter_lower, filter_upper):\n",
    "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
    "\n",
    "        if img_loss is not None:\n",
    "            processed_filters.append(img_loss)\n",
    "\n",
    "    print('{} filter processed.'.format(len(processed_filters)))\n",
    "    # Finally draw and store the best filters to disk\n",
    "    _draw_filters(processed_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets call the function and see what we get. \n",
    "_Remark:_ Sometimes the gradient ascent does not converge, so we do not get an image in this case. Also note that the images are put into a rectangular grid and only the best filters will bei included. The resulting image will be saved to disk. Of course you are free to change this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_layer(model, 'conv3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data with `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform),\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform),\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only trick here is the normalization. The mean and standard deviation passed in is the actual value computed for the dataset, after normalization (subtract and divide) the dataset will be a standard normal N(0,1) distribution. This just helps the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN architecture pytorch implementation, with `log_softmax()`at the final layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,   64,  3)\n",
    "        self.conv2 = nn.Conv2d(64,  128, 3)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out how many parameters the net has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "conv1.weight \t 1728\n",
      "conv1.bias \t 64\n",
      "conv2.weight \t 73728\n",
      "conv2.bias \t 128\n",
      "conv3.weight \t 294912\n",
      "conv3.bias \t 256\n",
      "fc1.weight \t 131072\n",
      "fc1.bias \t 128\n",
      "fc2.weight \t 32768\n",
      "fc2.bias \t 256\n",
      "fc3.weight \t 2560\n",
      "fc3.bias \t 10\n",
      "_____________________\n",
      "Total \t 537610\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "print('Trainable parameters:')\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, '\\t', param.numel())\n",
    "        total += param.numel()\n",
    "print('_____________________')\n",
    "print('Total', '\\t', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): # get the inputs; data is a list of [inputs, labels]\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # zero the parameter gradients\n",
    "        # forward + backward + optimize\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        # print statistics\n",
    "        if batch_idx > 0 and batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{}\\t({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return losses\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return (float(correct) / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [4000/50000\t(8%)]\tLoss: 2.299236\n",
      "Train Epoch: 0 [8000/50000\t(16%)]\tLoss: 2.827590\n",
      "Train Epoch: 0 [12000/50000\t(24%)]\tLoss: 1.893848\n",
      "Train Epoch: 0 [16000/50000\t(32%)]\tLoss: 1.790319\n",
      "Train Epoch: 0 [20000/50000\t(40%)]\tLoss: 1.316366\n",
      "Train Epoch: 0 [24000/50000\t(48%)]\tLoss: 1.193958\n",
      "Train Epoch: 0 [28000/50000\t(56%)]\tLoss: 0.779386\n",
      "Train Epoch: 0 [32000/50000\t(64%)]\tLoss: 1.118575\n",
      "Train Epoch: 0 [36000/50000\t(72%)]\tLoss: 0.553527\n",
      "Train Epoch: 0 [40000/50000\t(80%)]\tLoss: 0.873444\n",
      "Train Epoch: 0 [44000/50000\t(88%)]\tLoss: 1.363624\n",
      "Train Epoch: 0 [48000/50000\t(96%)]\tLoss: 0.831140\n",
      "\n",
      "Average loss: 1.2900, Accuracy: 26388/50000 (53%)\n",
      "\n",
      "Train Epoch: 1 [4000/50000\t(8%)]\tLoss: 0.789661\n",
      "Train Epoch: 1 [8000/50000\t(16%)]\tLoss: 1.121548\n",
      "Train Epoch: 1 [12000/50000\t(24%)]\tLoss: 1.102749\n",
      "Train Epoch: 1 [16000/50000\t(32%)]\tLoss: 1.690279\n",
      "Train Epoch: 1 [20000/50000\t(40%)]\tLoss: 1.345315\n",
      "Train Epoch: 1 [24000/50000\t(48%)]\tLoss: 1.151159\n",
      "Train Epoch: 1 [28000/50000\t(56%)]\tLoss: 0.364447\n",
      "Train Epoch: 1 [32000/50000\t(64%)]\tLoss: 1.060615\n",
      "Train Epoch: 1 [36000/50000\t(72%)]\tLoss: 0.742934\n",
      "Train Epoch: 1 [40000/50000\t(80%)]\tLoss: 2.057841\n",
      "Train Epoch: 1 [44000/50000\t(88%)]\tLoss: 1.200330\n",
      "Train Epoch: 1 [48000/50000\t(96%)]\tLoss: 1.070507\n",
      "\n",
      "Average loss: 0.9061, Accuracy: 34405/50000 (69%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device(\"cpu\") # or 'gpu'\n",
    "losses = []\n",
    "accuracies = []\n",
    "for epoch in range(0, 2): # loop over the dataset multiple times\n",
    "    losses.extend(train(model, device, train_loader, optimizer, epoch))\n",
    "    accuracies.append(test(model, device, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_label_predictions(model, device, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction))\n",
    "            predictions.extend(prediction)\n",
    "    return [i.item() for i in actuals], [i.item() for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8c72f83025ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactuals_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mactuals_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasslabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasslabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Confusion Matrix (train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8a6a97cd35e5>\u001b[0m in \u001b[0;36mtest_label_predictions\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ami/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ami/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ami/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ami/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEMFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    900\u001b[0m                         \u001b[0;34m\"Too many open files. Communication with the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                         \u001b[0;34m\" workers is no longer possible. Please increase the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
     ]
    }
   ],
   "source": [
    "actuals_train, predictions_train = test_label_predictions(model, device, train_loader)\n",
    "actuals_test, predictions_test = test_label_predictions(model, device, test_loader)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,5))\n",
    "plot_cm(actuals_train,predictions_train, ax=ax[0],classlabels=classlabels,cmtitle='Confusion Matrix (train)')\n",
    "plot_cm(actuals_test,predictions_test, ax=ax[1],classlabels=classlabels, cmtitle='Confusion Matrix (test)')\n",
    "\n",
    "print('F1 score train: %f' % f1_score(actuals_train, predictions_train, average='micro'))\n",
    "print('F1 score test: %f' % f1_score(actuals_test, predictions_test, average='micro'))\n",
    "\n",
    "print('Accuracy score train: %f' % accuracy_score(actuals_train, predictions_train))\n",
    "print('Accuracy score test: %f' % accuracy_score(actuals_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the ROC curve for the cat(3) class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_class_probabilities(model, device, test_loader, which_class):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    probabilities = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction) == which_class)\n",
    "            probabilities.extend(np.exp(output[:, which_class]))\n",
    "    return [i.item() for i in actuals], [i.item() for i in probabilities]\n",
    "\n",
    "which_class = 3\n",
    "actuals, class_probabilities = test_class_probabilities(model, device, test_loader, which_class)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(actuals, class_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC for label=cat(%d) class' % which_class)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
