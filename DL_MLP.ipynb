{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks in Keras\n",
    "\n",
    "From now on, we will start working with Keras and Neural Networks. \n",
    "This notebook uses the classic Auto MPG Dataset [Auto MPG Dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg) and builds a model to predict the fuel efficiency of late-1970s and early 1980s automobiles. To do this, provide the model with a description of many automobiles from that time period. This description includes attributes like: cylinders, displacement, horsepower, and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "___\n",
    "To develop our prediction model, we need to import the necessary Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.models import Sequential \n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from tensorflow.python.keras.layers import ReLU\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print('Keras Version: ' + keras.__version__)\n",
    "print('Tensorflow Version: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "___\n",
    "The dataset contains a few unknown values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"Origin\"` column is really categorical, not numeric. So convert that to a one-hot with pd.get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(dataset.iloc[:,1:], dataset.iloc[:,0], test_size = 0.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "____\n",
    "Have a quick look at the joint distribution of a few pairs of columns from the training set.\n",
    "\n",
    "Looking at the top row it should be clear that the fuel efficiency (MPG) is a function of all the other parameters. Looking at the other rows it should be clear that they are functions of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scx = StandardScaler()\n",
    "scy = StandardScaler()\n",
    "X_train = scx.fit_transform(X_train_raw)\n",
    "X_test = scx.transform(X_test_raw)\n",
    "y_train = (scy.fit_transform(y_train_raw.to_numpy().reshape(-1,1)))\n",
    "y_test = (scy.transform(y_test_raw.to_numpy().reshape(-1,1)))\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model\n",
    "___\n",
    "\n",
    "In Keras, the SequentialAPI can be used to define a neural network where layers can be \"stacked\" to each other. Each time Sequential.add() is called, a new layer gets attached to the end of the network. You have to define input and output shape only, the `SequentialAPI` does the job of defining the shapes for you in all intermediate layers. This API is easy to understand and very intuitive. In later tasks we will also cover the usage of the `FunctionalAPI`. \n",
    "\n",
    "An example for three fully connected (dense) feed forward layers - with 5, 2 and 1 neuron respectively - would be. For quick and easy exploration of different layer sizes we add a variable called `size_fac` which linearly scales the hidden layers. The output layer is not affected, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def build_model(act_fun='relu',loss_fun='mean_absolute_error',size_fac=1):\n",
    "    mlp = Sequential()\n",
    "    mlp.add(Dense(int(size_fac*5),input_shape=(X_train.shape[1],),activation='relu',name=\"hidden1\"))\n",
    "    mlp.add(Dense(int(size_fac*2),activation=act_fun,name=\"hidden2\")) \n",
    "    mlp.add(Dense(1,name=\"output\"))\n",
    "    mlp.compile(loss=loss_fun, optimizer=tf.keras.optimizers.Adam(0.001), metrics=[tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_error', 'mean_absolute_error'])\n",
    "    return mlp\n",
    "\n",
    "model = build_model()\n",
    "plot_model(model, show_shapes=True,show_layer_names=True,dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, the above call builds a neural network that has an input layer for 9 features, and then two hidden layers and an output layer. The build_function is used so that the model can be parametrized and grid searches can be performed. \n",
    " \n",
    "_Remark: We chose 3 layers somewhat arbitrary. It might also be an idea to use more neurons in a layer and a less deep net - for this particular dataset. In Feed Forward Networks ReLU is chosen often as activation function to serve as a starting point when developing a neural net. The Mean Squared Error (MSE) and Mean Absolute Error (MAE) are common loss functions used for regression problems. Mean Absolute Error is less sensitive to outliers. Different loss functions are used for classification problems._\n",
    "\n",
    "For teaching and small experiments it will always somehow be hard to provide deep learning examples due to the computational complexity. Always feel free to adopt parameters and/or code to let the examples run in adequate duration!\n",
    "\n",
    "Now that we have the build function, we can build and train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(act_fun='relu',loss_fun='mean_absolute_error',size_fac=4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    verbose=1, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(history, ax=None, metric='loss', ep_start=1, ep_stop=None,monitor='val_loss', mode='min',plttitle=None):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    if ep_stop is None:\n",
    "        ep_stop = len(history.epoch)\n",
    "    if plttitle is None:\n",
    "        plttitle = metric[0].swapcase() + metric[1:] + ' During Training'\n",
    "    ax.plot(np.arange(ep_start,ep_stop+1, dtype='int'),history.history[metric][ep_start-1:ep_stop])\n",
    "    ax.plot(np.arange(ep_start,ep_stop+1, dtype='int'),history.history['val_' + metric][ep_start-1:ep_stop])\n",
    "    ax.set(title=plttitle)\n",
    "    ax.set(ylabel=metric[0].swapcase() + metric[1:])\n",
    "    ax.set(xlabel='Epoch')\n",
    "    ax.legend(['train', 'val'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "plot_model_history(history,ax=ax[0])\n",
    "plot_model_history(history, metric='mean_squared_error',ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "____\n",
    "Now that the model is trained check the test-set performance and see how it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('test loss, test loss:', test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, predict have a look at the errors made by the model when making predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = scy.inverse_transform(model.predict(X_train))\n",
    "y_pred_test =  scy.inverse_transform(model.predict(X_test))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "ax = plt.axes(aspect='equal')\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "sns.regplot(x=y_train_raw.to_numpy(), y=y_pred_train.reshape(-1), ax=ax)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "ax = plt.subplot(122)\n",
    "sns.regplot(x=y_test_raw.to_numpy(), y=y_pred_test.reshape(-1), color='r', ax=ax)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Training, Preventing Overfitting, and Callbacks\n",
    "___\n",
    "\n",
    "Overfitting is something that you have to pay close attention to when designing and training neural networks. Especially for simpler problems, bigger networks tend to overfit quickly. \n",
    "An important tool to monitor this is passing validation data after each epoch or batch and plotting a curve of the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(act_fun='relu',loss_fun='mean_absolute_error',size_fac=30)\n",
    "history = model.fit(X_train, y_train,validation_split=0.25, batch_size=64, epochs=250,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "plot_model_history(history, ax=ax[0])\n",
    "ax[0].set_ylim(0.1,.35)\n",
    "plot_model_history(history, metric='root_mean_squared_error',ax=ax[1])\n",
    "ax[1].set_ylim(0.2,.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there's usually a sweet spot between training too little and training too much, and ideally we would want to stop training after reaching it. In keras, this can be done using an so called `EarlyStopping` callback. The _callback_list_ argument of the `model.fit()` function takes all so called callback functions. This functions will be called after each epoch.Below we indtroduce the `ModelCheckpoint` which enables us to save  You can define your very own callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model = build_model(act_fun='relu',loss_fun='mean_absolute_error',size_fac=3)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=15)\n",
    "mc = ModelCheckpoint(\"best_epoch.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [es,mc]\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=200, batch_size=32, callbacks=callbacks_list, verbose=0)\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('test loss, test loss:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = scy.inverse_transform(model.predict(X_train))\n",
    "y_pred_test =  scy.inverse_transform(model.predict(X_test))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "ax = plt.axes(aspect='equal')\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "sns.regplot(x=y_train_raw.to_numpy(), y=y_pred_train.reshape(-1), ax=ax)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "ax = plt.subplot(122)\n",
    "sns.regplot(x=y_test_raw.to_numpy(), y=y_pred_test.reshape(-1), color='r', ax=ax)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size and Convergence\n",
    "___\n",
    "\n",
    "You may have noticed by now that `model.fit()` in keras takes a parameter called`\"batch_size\"`. The batch is the number of samples that are passed through the network before a weight update is performed, and it can greatly affect your model's training. On the one hand, increasing the batch size can considerably speed up training time, on the other hand the model can take significantly more epochs until convergence if the batch size is too high (because there are less weight updates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "batch_sizes = [1,8,32,128]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "axs = axs.reshape(-1)\n",
    "\n",
    "ep = 100\n",
    "training_times = []\n",
    "\n",
    "for i,bs in enumerate(batch_sizes,0):\n",
    "    start = time.time()\n",
    "    print(\"Training model with batch size: %s\" %(bs))\n",
    "    model = build_model(act_fun='relu',loss_fun='mean_absolute_error',size_fac=3)\n",
    "    history = model.fit(X_train, y_train,validation_split=0.25, batch_size=bs, epochs=ep,verbose=0)\n",
    "    plt.sca(axs[i])\n",
    "    plot_model_history(history, ax=axs[i], ep_stop=ep, plttitle='Loss During Traning, BS=' + str(bs))\n",
    "    training_times.append(time.time()-start)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(10)\n",
    "fig.tight_layout(pad=1.0)\n",
    "\n",
    "fig2 = plt.figure(figsize=(5.5,3))\n",
    "sns.barplot(x=np.arange(0,4),y=training_times)\n",
    "plt.xticks(ticks=[0,1,2,3], labels=[\"1\",\"8\",\"32\",\"128\"])\n",
    "plt.xlabel(\"batch size\")\n",
    "plt.ylabel(\"Training time in seconds\")\n",
    "plt.title(\"Training times in relation to batch size\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model behaves quite differently. While training times are obviously longer with smaller batch sizes, you can also see how the smaller batch size allows for quicker convergence. A downside of this is also that the model with the smaller batch size overfits harder, since updates for larger batchsizes contain more different samples and thus might lead to more generalization.\n",
    "\n",
    "There is no right or wrong here, choosing the right batch size depends on your computing power and the size of your dataset, but common values are 4-16 for smaller datasets and CPU processing, and 64,128 or 256 for larger datasets and GPU processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Wrappers\n",
    "\n",
    "To integrate our keras models into the sklearn workflow, keras provides the `KerasRegressor()` wrapper, which enables us to use sci-kit methods with keras models.\n",
    "\n",
    "Here we build a `KerasRegressor()` from our keras model and train it using earlystopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=15)\n",
    "mc = ModelCheckpoint(\"best_epoch.h5\", monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [es,mc]\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=build_model, validation_split=0.25, epochs=200, batch_size=32, verbose=1,size_fac=6,act_fun='relu',loss_fun='mean_absolute_error')))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "wrapped_model = TransformedTargetRegressor(\n",
    "    regressor=pipeline,\n",
    "    transformer=StandardScaler())\n",
    "\n",
    "wrapped_model.fit(X_train_raw, y_train_raw, mlp__callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are mostly used in deep learning and processing a huge amount of training data. Therefore, cross-validation or exhaustive grid-search is not feasable. But having the sklearn wrapper and with our light-weight model we can even go for a cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "results = cross_val_score(wrapped_model, X_train_raw, y_train_raw, scoring='neg_root_mean_squared_error', cv=kfold, fit_params={'mlp__callbacks' : callbacks_list})\n",
    "print(\"RMSE: %.2f (+/- %.2f)\" % (-(results.mean()), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.regressor_['mlp'].get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
