{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76869961",
   "metadata": {},
   "source": [
    "# <span style='color:Blue'> STUDENT PERFORMANCE ANALYSIS </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cba7c",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "To develop our prediction model, we need to import the necessary Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f757d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import loguniform, sem\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, LeavePOut, ShuffleSplit, RepeatedKFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d339af9",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808a032",
   "metadata": {},
   "source": [
    "This dataset is from the UCI Machine Learning Repository and is comprised of student performance inforation (can be found by clicking the following link: https://archive.ics.uci.edu/ml/datasets/Student+Performance). The data contains the following features:\n",
    "<details>\n",
    "<summary>\n",
    "<a class=\"btnfire small stroke\"><em class=\"fas fa-chevron-circle-down\"></em>&nbsp;&nbsp;Description of the variables:</a>    \n",
    "</summary>\n",
    "\n",
    "    \n",
    "\n",
    "* `school` - student’s school (binary: ‘GP’ - Gabriel Pereira or ‘MS’ - Mousinho da Silveira)\n",
    "\n",
    "* `sex` - student’s sex (binary: ‘F’ - female or ‘M’ - male)\n",
    "\n",
    "* `age` - student’s age (numeric: from 15 to 22)\n",
    "\n",
    "* `address` - student’s home address type (binary: ‘U’ - urban or ‘R’ - rural)\n",
    "\n",
    "* `famsize` - family size (binary: ‘LE3’ - less or equal to 3 or ‘GT3’ - greater than 3)\n",
    "\n",
    "* `Pstatus` - parent’s cohabitation status (binary: ‘T’ - living together or ‘A’ - apart)\n",
    "\n",
    "* `Medu` - mother’s education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n",
    "\n",
    "* `Fedu` - father’s education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n",
    "* `Mjob` - mother’s job (nominal: ‘teacher’, ‘health’ care related, civil ‘services’ (e.g. administrative or police), ‘at_home’ or ‘other’)\n",
    "* `Fjob` - father’s job (nominal: ‘teacher’, ‘health’ care related, civil ‘services’ (e.g. administrative or police), ‘at_home’ or ‘other’)\n",
    "* `reason` - reason to choose this school (nominal: close to ‘home’, school ‘reputation’, ‘course’ preference or ‘other’)\n",
    "* `guardian` - student’s guardian (nominal: ‘mother’, ‘father’ or ‘other’)\n",
    "* `traveltime` - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "* `studytime` - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "* `failures` - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "* `schoolsup` - extra educational support (binary: yes or no)\n",
    "* `famsup` - family educational support (binary: yes or no)\n",
    "* `paid` - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "* `activities` - extra-curricular activities (binary: yes or no)\n",
    "* `nursery` - attended nursery school (binary: yes or no)\n",
    "* `higher` - wants to take higher education (binary: yes or no)\n",
    "* `internet` - Internet access at home (binary: yes or no)\n",
    "* `romantic` - with a romantic relationship (binary: yes or no)\n",
    "* `famrel` - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "* `freetime` - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "* `goout` - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "* `Dalc` - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "* `Walc` - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "* `health` - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "* `absences` - number of school absences (numeric: from 0 to 93)\n",
    "* `G1` - first period grade (numeric: from 0 to 20)\n",
    "* `G2` - second period grade (numeric: from 0 to 20)\n",
    "* `G3` - final grade (numeric: from 0 to 20, output target)\n",
    "</details>\n",
    "<br\\><br\\>\n",
    "    \n",
    "The value on which we try to make predictions is `G3`, represents the grade at the end of the year and is therefore the one that determines the success or failure of the school year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da6054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('student-mat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858cb25",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36981412",
   "metadata": {},
   "source": [
    "The summary of the data reveals that the dataset has multiple categorical variables that need to be encoded. For this purpose we are using the `LabelEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_le = LabelEncoder()\n",
    "for column in data[[\"school\", \"sex\", \"address\", \"famsize\", \"Pstatus\",\n",
    "                  \"Mjob\", \"Fjob\", \"reason\", \"guardian\", \"schoolsup\",\n",
    "                  \"famsup\", \"paid\", \"activities\", \"nursery\", \"higher\",\n",
    "                  \"internet\", \"romantic\"]].columns:\n",
    "    \n",
    "    data[column] = class_le.fit_transform(data[column].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb812f",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c5697",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "Before we make the model, we need to split the data into train dataset and test dataset. We will use the train dataset to train the linear regression model. The test dataset will be used as a comparasion and see the performance of our model. We will use 67% of the data as the training data and the rest of it as the testing data. If we would determine the performance of our model only on the training set, we would end up with a way too optimistc estimate of our model performance. A random split of 1/3 and 2/3 is not the only option how we can split the data. Change the `test_size` and see how it affects the perfomance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8979f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data,test_size=0.33, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce1c56",
   "metadata": {},
   "source": [
    "Splitting the dataset into the source variables (independant variables) and the target variable (dependant variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create X and Y\n",
    "X_train = train.iloc[:, :-1]\n",
    "Y_train = train.iloc[:, -1:]\n",
    "\n",
    "X_test = test.iloc[:, :-1]\n",
    "Y_test = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([('scaler', StandardScaler(), [\"age\", \"traveltime\", \"studytime\", \"failures\", \"famrel\",\n",
    "                  \"freetime\", \"goout\", \"Dalc\", \"Walc\", \"health\",\n",
    "                  \"absences\", \"G1\", \"G2\"])],remainder='passthrough')\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186e836",
   "metadata": {},
   "source": [
    "Scaling input variables is straightforward. In `scikit-learn`, you can use the scale objects manually, or the more convenient `Pipeline` that allows you to chain a series of data transform objects together before using your model.\n",
    "The `Pipeline` will fit the scale objects on the training data for you and apply the transform to new data, such as when using a model to make a prediction. If you want to try a more complicated apporach you can build your Pipeline with the `ColumnTransformer` above. The transformer ensures that only the consinuous vairables will be scaled and the categorical varaibles won't. One can argue about the sense of scaling binary variables which encode the sex or the school and so on. It might also be easier to interpret when not scaling such categorical variables - otherwise you could end up with a float number inidcating if a student is female or male. In the end you can make the desicision wether to scale or not to scale categorical variables by the predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2961c7a",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', ct),\n",
    "                 ('ridge_regression', Ridge(5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformedTargetRegressor(regressor=pipe, transformer=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6e274",
   "metadata": {},
   "source": [
    "### Make scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, predict):\n",
    "    predict = np.array(predict)\n",
    "    actual = np.array(actual)\n",
    "\n",
    "    distance = predict - actual\n",
    "\n",
    "    square_distance = distance ** 2\n",
    "\n",
    "    mean_square_distance = square_distance.mean()\n",
    "\n",
    "    score = np.sqrt(mean_square_distance)\n",
    "\n",
    "    return score\n",
    "\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9680b-6451-4722-971a-c4cba516df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(y_true, y_pred):\n",
    "    SPxy =  np.sum((y_true - np.mean(y_true))*(y_pred-np.mean(y_pred))) \n",
    "    SQx = np.sum(np.square(y_true - np.mean(y_true))) \n",
    "    SQy = np.sum(np.square(y_pred - np.mean(y_pred))) \n",
    "    return ( SPxy/(np.sqrt(SQx*SQy) + np.finfo(np.float64).eps))\n",
    "pearson_scorer = make_scorer(pearson_correlation, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ba22e",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "Cross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts (e.g. k = 10). The algorithm is trained on k − 1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_kf = KFold(n_splits=6,shuffle=True,random_state=1)\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(10, 6))\n",
    "fig.subplots_adjust(hspace = .5, wspace=.4)\n",
    "ax = axs.ravel()\n",
    "\n",
    "score = list()\n",
    "rmse_score = list()\n",
    "pearson_score = list()\n",
    "\n",
    "gl_min = Y_train.min()\n",
    "gl_max = Y_train.max()\n",
    "i = 0\n",
    "for train_ix, test_ix in cv_kf.split(X_train):\n",
    "    # split data\n",
    "    X_tr, X_te = X_train.iloc[train_ix, :], X_train.iloc[test_ix, :]\n",
    "    y_tr, y_te = Y_train.iloc[train_ix], Y_train.iloc[test_ix]\n",
    "    # fit and evaluate a model\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "    \n",
    "    rmse_score.append(rmse(y_te, preds))\n",
    "    score.append(model.score(X_te,y_te))\n",
    "    pearson_score.append(pearson_correlation(y_te,preds))\n",
    "    \n",
    "    ax[i].scatter(y_te, preds, edgecolors=(0, 0, 0))\n",
    "    #ax[i].plot([y_te.min(), y_te.max()], [y_te.min(), y_te.max()], 'k--', lw=2)\n",
    "    ax[i].plot([gl_min, gl_max], [gl_min, gl_max], 'k--', lw=2)\n",
    "    ax[i].set_xlabel('Observed')\n",
    "    ax[i].set_ylabel('Predicted')\n",
    "    ax[i].set_xlim([gl_min.values-2, gl_max.values+2])\n",
    "    ax[i].set_ylim([gl_min.values-2, gl_max.values+2])\n",
    "    ax[i].set_title(\"RMSE: %.3f\" % (rmse(preds, y_te)))\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Mean RMSE: %.3f and STD: +/- %.3f\" % (np.mean(rmse_score), np.std(rmse_score)))\n",
    "print(\"Mean R^2: %.3f and STD: +/- %.3f\" % (np.mean(score), np.std(score)))   \n",
    "print(\"Mean r_P: %.3f and STD: +/- %.3f\" % (np.mean(pearson_score), np.std(pearson_score))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50416be1",
   "metadata": {},
   "source": [
    "Now we want to see the effect of k on our performance estimate and run a \"sensitivity analysis\" for different k values. That is, to evaluate the performance of the same model on the same dataset with different values of k and see how they compare.\n",
    "Nevertheless, we can choose a test condition that represents an “ideal” or as best as we can achieve “ideal” estimate of model performance. Normaly the datasets are not big enough to take one more test set out. For this purpose we can use `LeaveOneOut()` CV method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using a given test condition\n",
    "def evaluate_model(model, X, y, cv):\n",
    "    scores = cross_val_score(model, X, y, scoring=rmse_scorer, cv=cv, n_jobs=-1)\n",
    "    # return scores\n",
    "    return -np.mean(scores), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ideal test condition\n",
    "ideal, _ = evaluate_model(model, X_train, Y_train, LeaveOneOut())\n",
    "print('Ideal: %.3f' % ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(1,23)\n",
    "folds = [2]\n",
    "for i in a:\n",
    "    #print(folds[i-1]+i)\n",
    "    folds.append(folds[i-1]+i)\n",
    "folds.append(264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean and min/max of each set of results\n",
    "means, stds = list(),list()\n",
    "# evaluate each k value\n",
    "for k in folds:\n",
    "    # define the test condition\n",
    "    cv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "    # evaluate k value\n",
    "    k_mean, k_std = evaluate_model(model, X_train, Y_train,cv)\n",
    "    # report performance\n",
    "    print('Folds=%d, RMSE=%.3f (+/- %.3f)' % (k, k_mean, k_std))\n",
    "    # store mean accuracy\n",
    "    means.append(k_mean)\n",
    "    stds.append(k_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(folds, means, yerr=stds, fmt='o')\n",
    "\n",
    "# plot the ideal case in a separate color\n",
    "#plt.plot(folds, [ideal for _ in range(len(folds))], color='r')\n",
    "\n",
    "# show the plot\n",
    "plt.title(\"Line plot of k mean values with error bars (std)\")\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9d77c",
   "metadata": {},
   "source": [
    "The choice of k is usually 5 or 10, but there is no formal rule. As k gets larger, the difference in size between the entire training set and the subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller. Also the number of k definitely affects the computational complexity almost linearly (asymptotically, linearly) for training algorithms with algorithmic complexity linear in the number of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32759eb",
   "metadata": {},
   "source": [
    "#### Repeated K-Fold Cross Validation\n",
    "\n",
    "This is where the k-fold cross-validation procedure is repeated `n_repeats` times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38499bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rep_kfold = RepeatedKFold(n_splits=10, n_repeats=3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = range(1,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_means, rep_stds = list(), list()\n",
    "\n",
    "for r in repeats:\n",
    "    # evaluate using a given number of repeats\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=r, random_state=10)\n",
    "    rep_mean, rep_std = evaluate_model(model, X_train, Y_train,cv)\n",
    "    # summarize\n",
    "    print('%d mean=%.4f standard_error=%.3f' % (r, rep_mean, rep_std))\n",
    "    # store\n",
    "    rep_means.append(rep_mean)\n",
    "    rep_stds.append(rep_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f8711-64fb-40ea-9d17-9032dfe80215",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(np.arange(1,r+1), rep_means, yerr=rep_stds, fmt='o')\n",
    "# show the plot\n",
    "plt.xlabel('repetitions')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2cf878",
   "metadata": {},
   "source": [
    "### Halving GridSearch CV vs GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb410a",
   "metadata": {},
   "source": [
    "The best way to find the optimal model hyperparameter is using cross-validation. However, instead of independently searching the hyperparameter set candidates, their successive halving search strategy “starts evaluating all the candidates with a small number of resources and iteratively selects the best candidates, using more and more resources.” The default resource is the number of samples, but it can be set to any positive-integer model parameter like gradient boosting rounds. Thus, the halving approach has the potential of finding good hyperparameters in less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbef917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "space = dict()\n",
    "space['regressor__ridge_regression__solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "space['regressor__ridge_regression__alpha'] = [1e-8,1e-5, 1e-4, 5e-4, 1e-3,5e-3, 1e-2, 5e-2, 1e-1,5e-1, 1,2,5,10,20,50, 100]\n",
    "#define cv\n",
    "cv_kfold = KFold(n_splits=10,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a85f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    " grid_search_params = dict( estimator = model,\n",
    "                            param_grid = space,\n",
    "                            scoring = rmse_scorer,\n",
    "                            return_train_score=True,\n",
    "                            cv=cv_kfold,\n",
    "                            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146de9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "FACTOR = 2\n",
    "MAX_RESOURCE_DIVISOR = 4\n",
    "\n",
    "n_samples = len(X_train)\n",
    "halving_results_n_samples =\\\n",
    "    HalvingGridSearchCV(resource='n_samples',\n",
    "                        min_resources=n_samples//\\\n",
    "                        MAX_RESOURCE_DIVISOR,\n",
    "                        factor=FACTOR,\n",
    "                        **grid_search_params\n",
    "                        )\\\n",
    "                        .fit(X_train, Y_train)\n",
    "\n",
    "pd.DataFrame(halving_results_n_samples.best_params_, index=[0])\\\n",
    "    .assign(RMSE=abs(halving_results_n_samples.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "full_results = GridSearchCV(**grid_search_params)\\\n",
    "               .fit(X_train, Y_train)\n",
    "\n",
    "pd.DataFrame(full_results.best_params_, index=[0])\\\n",
    "    .assign(RMSE=abs(full_results.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13526c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = full_results.best_estimator_\n",
    "clf_params = full_results.best_params_\n",
    "clf_score = abs(full_results.best_score_)\n",
    "clf_stdev = full_results.cv_results_['std_test_score'][full_results.best_index_]\n",
    "cv_results = full_results.cv_results_\n",
    "\n",
    "print(\"best parameters: {}\".format(clf_params))\n",
    "print(\"best score:      {:0.5f} (+/-{:0.5f})\".format(clf_score, clf_stdev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_names = []\n",
    "[ coef_names.append('coef_' + str(x)) for x in range(X_train.shape[1]) ]\n",
    "cfs = pd.DataFrame(columns=coef_names)\n",
    "for train_index, test_index in cv_kfold.split(X_train):\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "    y_train_cv, y_test_cv = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "    cve = full_results.best_estimator_.fit(X_train_cv,y_train_cv)\n",
    "    coefs = cve.regressor_['ridge_regression'].coef_\n",
    "    cfs = cfs.append(pd.DataFrame(coefs.reshape(1,-1),columns=coef_names))\n",
    "cfs.index = np.arange(0, len(cfs))\n",
    "g = sns.catplot(data=cfs)\n",
    "g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ee7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out the best results\n",
    "# =========================\n",
    "scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "\n",
    "best_row = scores_df.iloc[0, :]\n",
    "best_mean = -best_row['mean_test_score']\n",
    "best_stdev = best_row['std_test_score']\n",
    "best_param = best_row['param_' + 'regressor__ridge_regression__alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# ================\n",
    "scores_df = scores_df.sort_values(by='param_' + 'regressor__ridge_regression__alpha')\n",
    "\n",
    "means = -scores_df['mean_test_score']\n",
    "\n",
    "stds = scores_df['std_test_score']\n",
    "params = scores_df['param_' + 'regressor__ridge_regression__alpha']\n",
    "\n",
    "# plot\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.errorbar(params, means, yerr=stds)\n",
    "\n",
    "plt.axhline(y=best_mean + best_stdev, color='red')\n",
    "plt.axhline(y=best_mean - best_stdev, color='red')\n",
    "plt.plot(best_param, best_mean, 'or')\n",
    "\n",
    "plt.title('regressor__ridge_regression__alpha' + \" vs Score\\nBest Score {:0.5f}\".format(clf_score))\n",
    "plt.xlabel('regressor__ridge_regression__alpha')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = full_results.best_estimator_.predict(X_train)\n",
    "Y_pred_test = full_results.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff19558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse(Y_pred_train,Y_train.values))\n",
    "print(rmse(Y_pred_test,Y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd02b2b",
   "metadata": {},
   "source": [
    "### Nested Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48540c",
   "metadata": {},
   "source": [
    "The cross-validation procedure is used to estimate the performance of machine learning models when making predictions or classify the data.\n",
    "\n",
    "This procedure can be used both when optimizing the hyperparameters of a model on a dataset, and when comparing and selecting a model for the dataset. When the same cross-validation procedure and dataset are used to both tune and select a model, it is likely to lead to an optimistically biased evaluation of the model performance.\n",
    "\n",
    "One approach to overcoming this bias is to nest the hyperparameter optimization procedure under the model selection procedure. This is called double cross-validation or nested cross-validation and is the preferred way to evaluate and compare tuned machine learning models. The nested CV has an inner loop CV nested in an outer CV. The inner loop is responsible for model selection/hyperparameter tuning (similar to validation set), while the outer loop is for error estimation (test set).\n",
    "\n",
    "In the inner loop (ex. `GridSearchCV`), we try to find good parameters using the \"inner test set\" aka validation \"set\". In the outer loop (ex. `cross_val_score`), the generalization error is estimated by averaging test set scores over several dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"Training RMSE: {:.2f}\".format(rmse(Y_pred_train, Y_train))) \n",
    "print(\"Test RMSE: {:.2f}\".format(rmse(Y_pred_test, Y_test)))\n",
    "print(\"Training R2: {:.2f}\".format(r2_score(Y_train, Y_pred_train))) \n",
    "print(\"Test R2: {:.2f}\".format(r2_score(Y_test, Y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random trials\n",
    "NUM_TRIALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to store scores\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Loop for each trial\n",
    "for i in range(NUM_TRIALS):\n",
    "\n",
    "    # Choose cross-validation techniques for the inner and outer loops,\n",
    "    # independently of the dataset.\n",
    "    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "    print(\"Trial \"+str(i))\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    clf = GridSearchCV(estimator=model, param_grid=space, cv=inner_cv)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    non_nested_scores[i] = clf.best_score_\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_score = cross_val_score(clf, X=X_train, y=Y_train, cv=outer_cv)\n",
    "    nested_scores[i] = nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_difference = non_nested_scores - nested_scores\n",
    "\n",
    "# Plot scores on each trial for nested and non-nested CV\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "non_nested_scores_line, = plt.plot(non_nested_scores, 'rs')\n",
    "nested_line, = plt.plot(nested_scores, 'bd')\n",
    "plt.ylabel(\"score\", fontsize=\"14\")\n",
    "plt.legend([non_nested_scores_line, nested_line],\n",
    "           [\"Non-Nested CV\", \"Nested CV\"],\n",
    "           bbox_to_anchor=(0, .4, .5, 0))\n",
    "plt.title(\"Non-Nested and Nested Cross Validation\",\n",
    "          x=.5, y=1.1, fontsize=\"15\")\n",
    "\n",
    "# Plot bar chart of the difference.\n",
    "plt.subplot(212)\n",
    "difference_plot = plt.bar(range(NUM_TRIALS), score_difference)\n",
    "plt.xlabel(\"Individual Trial #\")\n",
    "plt.legend([difference_plot],\n",
    "           [\"Non-Nested CV - Nested CV Score\"],\n",
    "           bbox_to_anchor=(0, 1, .8, 0))\n",
    "plt.ylabel(\"score difference\", fontsize=\"14\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdb664",
   "metadata": {},
   "source": [
    "Nested CV effectively uses a series of `train`,`validation` and `test` set splits. In the inner loop (here executed by GridSearchCV), the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting hyperparameters over the validation set. In the outer loop (here in cross_val_score), generalization error is estimated by averaging test set scores over several dataset splits.\n",
    "We compare the performance of non-nested and nested CV strategies by taking the difference between their scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1bb69",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "First of all it can be interesting how the grades of the students contained in these datasets are distributed so that we can better understand the results. Following the suggestion of [Paulo Cortez and Alice Silva's paper](http://www3.dsi.uminho.pt/pcortez/student.pdf)  the student grades can be analysed using 5-Level classification based on the Erasmus grade conversion system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a47e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_transform(g):\n",
    "    if g >15:\n",
    "        return 1\n",
    "    elif g<16 and g>13: \n",
    "        return 2\n",
    "    elif g<14 and g>11:\n",
    "        return 3\n",
    "    elif g<12 and g>9:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "\n",
    "data_c = data.copy()\n",
    "data_c['grades'] = data_c.apply(lambda x: grade_transform(x['G3']), axis = 1 )\n",
    "\n",
    "\n",
    "data_c['grades'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012936b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c.drop('G3', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "data_c['grades'] = le.fit_transform(data_c['grades'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb07ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(data_c.iloc[:,:-1],data_c.iloc[:,-1],test_size=0.33,\n",
    "                                                           random_state=1, stratify=data_c.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116e84d-a0a4-4288-952f-06af92244d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "sns.histplot(data_c.iloc[:,-1],kde=False,label='All', ax=ax)\n",
    "sns.histplot(y_train_c+.05, kde=False, label='train', color='green', ax=ax)\n",
    "sns.histplot(y_test_c+.05, kde=False, label='test', color='orange', ax=ax)\n",
    "plt.xlabel('G3')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks([0.25,1,2.25,3,3.75], ['1','2','3','4','5'])\n",
    "plt.title(\"Distribution of Classes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77076ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_2 = LogisticRegression(penalty='l1', \n",
    "                           dual=False, \n",
    "                           tol=0.001, \n",
    "                           C=0.00001, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1,\n",
    "                           solver='saga',\n",
    "                           class_weight=None, \n",
    "                           random_state=1, \n",
    "                           max_iter=1000000, \n",
    "                           multi_class='auto', \n",
    "                           verbose=0, \n",
    "                           warm_start=False, \n",
    "                           n_jobs=1)\n",
    "\n",
    "\n",
    "model_logreg = Pipeline(steps=[('StandardScaler', StandardScaler()), ('LogisticRegression', clf_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41655362",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, pred_test = [], []\n",
    "\n",
    "intervals = np.arange(10, X_train_c.shape[0], 10)\n",
    "\n",
    "for i in intervals:\n",
    "    model_logreg.fit(X_train_c.iloc[:i,:], y_train_c.values[:i,])\n",
    "    #print(i)\n",
    "    p_train = model_logreg.score(X_train_c.iloc[:i,:], y_train_c.iloc[:i,])\n",
    "    p_test = model_logreg.score(X_test_c.iloc[:i,:], y_test_c.iloc[:i,])\n",
    "    pred_train.append(p_train)\n",
    "    pred_test.append(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf633a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with plt.style.context(('fivethirtyeight')):\n",
    "plt.plot(intervals, pred_train, marker='o', label='Train')\n",
    "plt.plot(intervals, pred_test, marker='s', label='Test')\n",
    "plt.legend(loc='best', numpoints=1)\n",
    "plt.xlim([0, X_train_c.shape[0]+30])\n",
    "plt.axvspan(X_train_c.shape[0], \n",
    "            X_train_c.shape[0] + X_test_c.shape[0], \n",
    "            alpha=0.2, \n",
    "            color='steelblue')\n",
    "plt.ylim([0.1, .70])\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e5438",
   "metadata": {},
   "source": [
    "Finding a good balance between bias and variance is important for model evaluation and selection.\n",
    "The reason why a proportionally large test sets increase the pessimistic bias is that the model may not have reached its full capacity, yet. In other words, the learning algorithm could have formulated a more powerful, more generalizable hypothesis for classification if it had seen more data. [Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning](https://arxiv.org/pdf/1811.12808.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f82f10",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "model_logreg.fit(X_train_c, y_train_c)\n",
    "\n",
    "y_pred_train_c = model_logreg.predict(X_train_c)\n",
    "y_pred_test_c = model_logreg.predict(X_test_c)\n",
    "\n",
    "conf_train = confusion_matrix(y_train_c,y_pred_train_c)\n",
    "conf_test = confusion_matrix(y_test_c,y_pred_test_c)\n",
    "\n",
    "fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "ax1.set_xticklabels(['1','2','3','4','5'])\n",
    "ax1.set_yticklabels(['1','2','3','4','5'])\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "ax2.set_xticklabels(['1','2','3','4','5'])\n",
    "ax2.set_yticklabels(['1','2','3','4','5'])\n",
    "ax2.set(title=\"Confusion Matrix for test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg={\"LogisticRegression__C\":np.logspace(-3,3,7), \"LogisticRegression__penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0212b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv=GridSearchCV(model_logreg,grid_logreg,cv=cv_kfold)\n",
    "logreg_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv.fit(X_train_c,y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_cv.best_params_)\n",
    "print(logreg_cv.best_score_)\n",
    "logreg_results = pd.DataFrame(logreg_cv.cv_results_).sort_values(by='rank_test_score')\n",
    "logreg_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_test = logreg_cv.decision_function(X_test_c)\n",
    "y_score_train = logreg_cv.decision_function(X_train_c)\n",
    "y_pred_train_c = logreg_cv.predict(X_train_c)\n",
    "y_pred_test_c = logreg_cv.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_train = confusion_matrix(y_train_c,y_pred_train_c)\n",
    "conf_test = confusion_matrix(y_test_c,y_pred_test_c)\n",
    "\n",
    "fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set_xticklabels(['1','2','3','4','5'])\n",
    "ax1.set_yticklabels(['1','2','3','4','5'])\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "ax2.set_xticklabels(['1','2','3','4','5'])\n",
    "ax2.set_yticklabels(['1','2','3','4','5'])\n",
    "ax2.set(title=\"Confusion Matrix for test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_data_c_ros, y_data_c_ros = ros.fit_resample(data_c.iloc[:,:-1], data_c.iloc[:,-1])\n",
    "X_train_c_ros, y_train_c_ros = ros.fit_resample(X_train_c, y_train_c)\n",
    "X_test_c_ros, y_test_c_ros = X_test_c, y_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f87ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "sns.histplot(y_data_c_ros,kde=False,label='All', ax=ax)\n",
    "sns.histplot(y_train_c_ros+.05, kde=False, label='train', color='green', ax=ax)\n",
    "sns.histplot(y_test_c_ros+.05, kde=False, label='test', color='orange', ax=ax)\n",
    "plt.xlabel('G3')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks([0.25,1,2.25,3,3.75], ['1','2','3','4','5'])\n",
    "plt.title(\"Distribution of Classes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd499b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg_ros = Pipeline(steps=[('StandardScaler', StandardScaler()),('over',RandomOverSampler()),('LogisticRegression', logreg_cv)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg_ros.fit(X_train_c_ros, y_train_c_ros)\n",
    "\n",
    "y_pred_train_c_ros = model_logreg_ros.predict(X_train_c_ros)\n",
    "y_pred_test_c_ros = model_logreg_ros.predict(X_test_c_ros)\n",
    "\n",
    "conf_train_ros = confusion_matrix(y_train_c_ros,y_pred_train_c_ros)\n",
    "conf_test_ros = confusion_matrix(y_test_c_ros,y_pred_test_c_ros)\n",
    "\n",
    "fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(conf_train_ros, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set_xticklabels(['1','2','3','4','5'])\n",
    "ax1.set_yticklabels(['1','2','3','4','5'])\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test_ros, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "ax2.set_xticklabels(['1','2','3','4','5'])\n",
    "ax2.set_yticklabels(['1','2','3','4','5'])\n",
    "ax2.set(title=\"Confusion Matrix for test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71a02d",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "The idea behind the bootstrap is to generate \"new samples\" by sampling from an empirical distribution. [Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning](https://arxiv.org/pdf/1811.12808.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "\n",
    "idx = np.arange(Y_train.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    train_idx = rng.choice(idx, size=idx.shape[0], replace=True)\n",
    "    \n",
    "    test_idx = np.setdiff1d(idx, train_idx, assume_unique=False)\n",
    "    \n",
    "    boot_train_X, boot_train_y = X_train.iloc[train_idx,:], Y_train.iloc[train_idx,:]\n",
    "    boot_test_X, boot_test_y = X_train.iloc[test_idx,:], Y_train.iloc[test_idx,:]\n",
    "    \n",
    "    model.fit(boot_train_X, boot_train_y)\n",
    "    pred = model.predict(boot_test_X)\n",
    "    acc = model.score(boot_test_X, boot_test_y)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "\n",
    "mean = np.mean(accuracies)\n",
    "\n",
    "#se = np.sqrt( (1. / (100-1)) * np.sum([(acc - mean)**2 for acc in accuracies])) \n",
    "#ci = 1.984 * se\n",
    "\n",
    "se = np.sqrt( (1. / (200-1)) * np.sum([(acc - mean)**2 for acc in accuracies])) \n",
    "ci = 1.97 * se\n",
    "\n",
    "lower = np.percentile(accuracies, 2.5)\n",
    "upper = np.percentile(accuracies, 90)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.vlines(mean, [0], 70, lw=2.5, linestyle='-', label='mean')\n",
    "#ax.vlines(med, [0], 60, lw=2.5, linestyle='--', label='median')\n",
    "ax.vlines(lower, [0], 20, lw=2.5, linestyle='-.', label='CI95 percentile')\n",
    "ax.vlines(upper, [0], 40, lw=2.5, linestyle='-.')\n",
    "\n",
    "ax.vlines(mean + ci, [0], 40, lw=2.5, linestyle=':', label='CI95 standard')\n",
    "ax.vlines(mean - ci, [0], 20, lw=2.5, linestyle=':')\n",
    "\n",
    "\n",
    "ax.hist(accuracies, bins=7,\n",
    "        color='#0080ff', edgecolor=\"none\", \n",
    "        alpha=0.3)\n",
    "plt.legend(loc='upper left')\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xlim([0.7, 0.9])\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
