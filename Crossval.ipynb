{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980ad4e0-0254-40eb-9fa6-345770d15731",
   "metadata": {},
   "source": [
    "## Live Notebook 1 - Validation (one more time)\n",
    "\n",
    "This notebook is intended to be worked on during the online session in individual breakout rooms. The focus is less on generating new code and more on experimenting with the existing layout. Interesting - unexpected as well as expected - results should be recorded by the groups directly in the notebook.\n",
    "In the last sessions we talked about Train/Test method, resampling, k-Fold corss-validation and bootstrapping. In this notebook we will do our own experiment(s) to investigate those methods a bit more detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b293c54-9221-4e33-b051-7eb6e4de3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_multilabel_classification\n",
    "from sklearn.model_selection import KFold, train_test_split, ShuffleSplit,cross_validate\n",
    "from mlxtend.evaluate import BootstrapOutOfBag\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ad4b6-b68b-43b6-895f-ccdb3e5bd628",
   "metadata": {},
   "source": [
    "### Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e96326-b157-47e2-a042-5ded087d9a96",
   "metadata": {},
   "source": [
    "At first we will define the parameters for the generatitve model. In this case it will be a very simple linear model, where we can vary the number of samples `n_samp`, the number of features `n_feat`, the number of informative features `n_inf`, which enables us to simulate bad chosen features, the standard deviation of gaussian noise `noise_std` and `bias`. This is where you have to try to generate different artificial datasets to see how the data itself influences the output of different validation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246938c1-a861-4185-96d9-3ab6d21355b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samp = 1000\n",
    "n_feat = 19\n",
    "n_inf = 18\n",
    "noise_std = 5 \n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfaacb-c650-4457-8669-753cbcf6814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, true_coef = make_regression(n_samples=n_samp, n_features=n_feat, random_state=0, noise=noise_std,bias=bias,n_informative=n_inf, coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34003ff-e4a0-4f01-9117-a732589604f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly split train and test\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc8828-aa6f-4740-9de5-9b9bb318e20d",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59711d05-7f81-43b1-a444-fa8a0a0edd9b",
   "metadata": {},
   "source": [
    "To keep the experiments handy and clear, we will use `LinearRegression()` as estimator for our experiments. This allows us to focus on the performance estimation without having to tune hyper-parameters. But you can replace the LR with different algorithms (bearning in mind to set fixed hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45187d0d-4735-41ec-95c8-67774dba6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "#reg = DummyRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6740e-0a6a-474a-b038-45910f9984bc",
   "metadata": {},
   "source": [
    "### Validation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a738073-b495-4ffb-bef7-9b08d49ea69d",
   "metadata": {},
   "source": [
    "For an easier interpretation and comparison we will use the same number of splits for all different validation methods (except for the train/test split). In the cell below we adress typical values (k=5 and k=10)for k as well as the most \"extreme\" choices (k=2 and k=n_samples). Despite those values you can specify stepsize, start and endpoint for k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e6323-d911-47aa-ad0f-11e2a767d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 800 #second largest k (max for generating the grid)\n",
    "ks = np.array([2,5,10]) \n",
    "ks = np.append(ks,np.arange(15,k_max, 30)) #grid of k values\n",
    "ks = np.append(ks,X.shape[0]) #k=n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2f899-bfce-4a43-9be0-99061334ee40",
   "metadata": {},
   "source": [
    "#### K Fold CV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904084f-08e5-4015-9b19-2715ddb6ebbf",
   "metadata": {},
   "source": [
    "Now we are ready to perform our experiments. We set up a dataframe for easier plotting afterwards and store every single result. The number of folds differes, therefore we fill the columns with `np.nan`, which will later be ignored when computing statistics or ding plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d4bab-ae1b-4bf2-84a9-52b167788ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Fold on X and y (aka \"full\" dataset) to estimate performance\n",
    "kf_df = pd.DataFrame()\n",
    "kf_cf = pd.DataFrame()\n",
    "for kk in ks:\n",
    "    cv = KFold(kk, shuffle=True, random_state=2)\n",
    "    c_v = cross_validate(reg, X, y,cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, return_estimator=True)\n",
    "    n = np.empty((np.max(ks)-kk))\n",
    "    n[:] = np.nan\n",
    "    kf_df['k=' + str(kk)] = (np.append(-c_v['test_score'], n)).tolist()\n",
    "    tmp = pd.DataFrame([est.coef_ for est in c_v['estimator']])\n",
    "    tmp['k'] = kk*np.ones((len(tmp)))\n",
    "    kf_cf = kf_cf.append(tmp)\n",
    "kf_cf.columns = ['coef_' + str(i) for i in range(n_feat)] + ['k']\n",
    "kf_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d58d9b-31fa-49e1-b41e-17d85190ea98",
   "metadata": {},
   "source": [
    "#### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b480a08-2e00-4858-8100-c8854724a82b",
   "metadata": {},
   "source": [
    "In resampling we also have to decide for a split ratio between train and test sets. We could also vary this parameter through the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d00a7-52a8-4417-8bd2-ef91e6c912a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampling on X and y (aka \"full\" dataset) to estimate performance\n",
    "re_df = pd.DataFrame()\n",
    "re_cf = pd.DataFrame()\n",
    "test_ratio = .3\n",
    "for kk in ks:\n",
    "    cv = ShuffleSplit(n_splits=kk, test_size=test_ratio, random_state=350)\n",
    "    c_v = cross_validate(reg, X, y,cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, return_estimator=True)\n",
    "    n = np.empty((np.max(ks)-kk))\n",
    "    n[:] = np.nan\n",
    "    re_df['splits=' + str(kk)] = (np.append(-c_v['test_score'], n)).tolist()\n",
    "    tmp = pd.DataFrame([est.coef_ for est in c_v['estimator']])\n",
    "    tmp['splits'] = kk*np.ones((len(tmp)))\n",
    "    re_cf = re_cf.append(tmp)\n",
    "re_cf.columns = ['coef_' + str(i) for i in range(n_feat)] + ['splits']\n",
    "re_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a3f88-30c6-4624-83b3-c9ad3262e3db",
   "metadata": {},
   "source": [
    "#### Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025e8f6-d59d-4b1e-a257-850a7287660e",
   "metadata": {},
   "source": [
    "Bootstrapping is less prominently used in Machine Learning than in classical statistics, maybe it is the reason that there is no direct sklearn implemenation (anymore). We use **mlxtend**'s bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ff21a-6dc5-441d-bd6a-0484cb2acefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrapping on X and y (aka \"full\" dataset) to estimate performance\n",
    "bs_df = pd.DataFrame()\n",
    "bs_cf = pd.DataFrame()\n",
    "for kk in ks:\n",
    "    cv = BootstrapOutOfBag(n_splits=int(kk), random_seed=456)\n",
    "    c_v = cross_validate(reg, X, y,cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, return_estimator=True)\n",
    "    n = np.empty((np.max(ks)-kk))\n",
    "    n[:] = np.nan\n",
    "    bs_df['splits=' + str(kk)] = (np.append(-c_v['test_score'], n)).tolist()\n",
    "    tmp = pd.DataFrame([est.coef_ for est in c_v['estimator']])\n",
    "    tmp['splits'] = kk*np.ones((len(tmp)))\n",
    "    bs_cf = bs_cf.append(tmp)\n",
    "bs_cf.columns = ['coef_' + str(i) for i in range(n_feat)] + ['splits']\n",
    "bs_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76e58c-6dc9-4992-bb54-4e3e2ad37f8a",
   "metadata": {},
   "source": [
    "#### Single Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272e231-59df-4360-a53e-075dba56e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on whole dataset (no test set at all)\n",
    "y_pred = reg.fit(X_train,y_train).predict(X_test)\n",
    "test_error = mean_squared_error(y_test,y_pred)\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151dafd2-9d70-4eb2-bd69-8f824f22f13a",
   "metadata": {},
   "source": [
    "#### No Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe21b3-0652-4fa8-882b-07c3af2e2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on whole dataset (no test set at all)\n",
    "y_pred = reg.fit(X,y).predict(X)\n",
    "train_error = mean_squared_error(y,y_pred)\n",
    "train_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01566063-fd98-4986-b311-da809ea7dd77",
   "metadata": {},
   "source": [
    "#### True Error\n",
    "As we generated the data by a pre-defined model, we can caclulate the best possible performance (of a linear model) by miltuplying the features with the coefficients and adding the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14def78-179f-44ee-84ef-7dbdd395a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True model on \"full data set\"\n",
    "true_error = mean_squared_error(y,bias+np.matmul(X,true_coef))\n",
    "true_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acfad7-4e71-4b90-9b66-8666bffb356f",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Now we can plot the reults from our experiments. As we recorded each error, we calculate the mean and the 95% confidence interval of the MSE's from different folds/splits/rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdc85b-8168-43f8-aa60-615f02dfa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
    "sns.pointplot(data=kf_df, estimator=np.nanmean, ci=95, ax=axs[0])\n",
    "sns.pointplot(data=re_df,estimator=np.nanmean, ci=95, ax=axs[1])\n",
    "sns.pointplot(data=bs_df,estimator=np.nanmean, ci=95, ax=axs[2])\n",
    "\n",
    "axs[0].set_title('k-Fold')\n",
    "axs[1].set_title('Resampling')\n",
    "axs[2].set_title('Bootstrap')\n",
    "\n",
    "# find min and max values for the plot\n",
    "minmax = [true_error,train_error, test_error]\n",
    "for ax in axs: \n",
    "    minmax = minmax + list((ax.get_yaxis().get_data_interval()))\n",
    "\n",
    "for ax in axs: \n",
    "    ax.plot([0,len(ks)-1],[true_error,true_error],'--r', label='true model')\n",
    "    ax.plot([0,len(ks)-1],[train_error,train_error],':k', label='no split')\n",
    "    ax.plot([0,len(ks)-1],[test_error,test_error],':m', label='test-set method')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=70)\n",
    "    ax.set_ylim(np.min(minmax)-.2,np.max(minmax)+.2)\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6e7f9-54b2-47f7-ac06-483a50aef7d6",
   "metadata": {},
   "source": [
    "We can also check the standard deviation of the regression coefficients throughout the different splits in the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccdd73-5ae3-414c-b8c7-5379dc4c8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(14,4))\n",
    "\n",
    "low_lim=0\n",
    "hi_lim = np.max([kf_cf.groupby('k').std().max().max(), re_cf.groupby('splits').std().max().max(), bs_cf.groupby('splits').std().max().max()])\n",
    "cmap = 'Blues'\n",
    "\n",
    "sns.heatmap(kf_cf.groupby('k').std(), ax=axs[0],vmin=low_lim, vmax=hi_lim, cmap=cmap,cbar_kws={'label': 'std of coef'})\n",
    "axs[0].set_title('k-Fold')\n",
    "axs[0].set_yticklabels(axs[0].get_yticklabels(),rotation=0)\n",
    "\n",
    "sns.heatmap(re_cf.groupby('splits').std(), ax=axs[1],vmin=low_lim, vmax=hi_lim,cmap=cmap,cbar_kws={'label': 'std of coef'})\n",
    "axs[1].set_title('Resampling')\n",
    "axs[1].set_yticklabels(axs[1].get_yticklabels(),rotation=0)\n",
    "sns.heatmap(bs_cf.groupby('splits').std(), ax=axs[2],vmin=low_lim, vmax=hi_lim,cmap=cmap,cbar_kws={'label': 'std of coef'})\n",
    "axs[2].set_title('Bootstrap')\n",
    "axs[2].set_yticklabels(axs[2].get_yticklabels(),rotation=0)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ff11f-e3e4-473d-bd0d-19becb458814",
   "metadata": {},
   "source": [
    "If you still have not had enough, you can look at the distributions (of the dependent variable) for individual training and test splits in a final step of this notebook. In this notebook we have only used random splits, so it is possible that the distribution of the training and test set looks different. If you use the test set method with a single test set, there may be larger deviations depending on the data set. Here we refer again to the reading (Applied Predictive Modeling - Chapter 4, or the paper of Westad et al) that one should think when splitting in particular also of the feature space and not only of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e3544-f065-4b91-a6f8-10b6df833749",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk=5 #k or number of splits\n",
    "train_inds=[]\n",
    "test_inds=[]\n",
    "\n",
    "cv_kf = KFold(kk, shuffle=True, random_state=2)\n",
    "cv_re = ShuffleSplit(n_splits=kk, test_size=test_ratio, random_state=350)\n",
    "cv_bs = BootstrapOutOfBag(kk, random_seed=456)\n",
    "cvs = [cv_kf,cv_bs,cv_re, cv_bs]\n",
    "\n",
    "for ct in range(3):\n",
    "    train_idx, test_idx = [],[]\n",
    "    for train_index, test_index in cvs[ct].split(X, y):\n",
    "        train_idx.append(train_index)\n",
    "        test_idx.append(test_index)\n",
    "    train_inds.append(train_idx)\n",
    "    test_inds.append(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06817eba-d5d0-42d0-af6e-5a70ca4dc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=4 #particular fold to visualize\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14,4))\n",
    "axs[0].set_title('k-Fold - fold=' + str(fold))\n",
    "axs[1].set_title('Resampling - fold=' + str(fold))\n",
    "axs[2].set_title('Bootstrapping - fold=' + str(fold))\n",
    "for ct in range(3):\n",
    "    dest=pd.DataFrame()\n",
    "    dset = pd.DataFrame(data={'set': ['train']*len(train_inds[ct][fold]),'y': y[train_inds[ct][fold]]})\n",
    "    dset = dset.append(pd.DataFrame(data={'set': ['test']*len(test_inds[ct][fold]),'y': y[test_inds[ct][fold]]}))\n",
    "    sns.kdeplot(data=dset, x=\"y\", hue=\"set\", ax=axs[ct])\n",
    "    axs[ct].grid()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
