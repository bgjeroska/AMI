{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting\n",
    "\n",
    "A time series is an array of data which consists of a an array of points, each of which contains one or more features. The crucial characteristic of time series is that we use them to model temporal dependencies, meaning that we want to perform operations based on the series.\n",
    "Example tasks for time series would be \n",
    "- predicting future stock prices based on current stock prices (univariate time series prediction)\n",
    "- predicting future weather based on current temperature, humidity and other weather (multivariate time series prediction)\n",
    "- Classifying the emotion of a piece of music (classifying a whole time series)\n",
    "- Classifying changes in emotion over multiple scenes (classifying each step of a time series).\n",
    "\n",
    "For all of these tasks Recurrent Neural Networks (RNN) can be very useful, since the RNN tries to capture the temporal component of the data by feeding each timestep into the network one by one. Especially LSTMs, which include both a \"short-term\" and a \"long-term\" memory state or even the simpler GRUs (Gatet Recurrent Units) are widley used. \n",
    "\n",
    "In this notebook, you will be working on a modified version of the Air Pollution Dataset, which contains weather data and air pollution measurements done in Peking, China. Your task will be forecasting the pollution of the next measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Input, GRU, Dense,Flatten,Dropout,Conv1D, GlobalAveragePooling1D, LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pmdarima import auto_arima\n",
    "import seaborn as sns\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(history, ax=None, metric='loss', ep_start=1, ep_stop=None, monitor='val_loss', mode='min', plttitle=None):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    if ep_stop is None:\n",
    "        ep_stop = len(history.epoch)\n",
    "    if plttitle is None:\n",
    "        plttitle = metric[0].swapcase() + metric[1:] + ' During Training'\n",
    "    ax.plot(np.arange(ep_start,ep_stop+1, dtype='int'),history.history[metric][ep_start-1:ep_stop])\n",
    "    ax.plot(np.arange(ep_start,ep_stop+1, dtype='int'),history.history['val_' + metric][ep_start-1:ep_stop])\n",
    "    ax.set(title=plttitle)\n",
    "    ax.set(ylabel=metric[0].swapcase() + metric[1:])\n",
    "    ax.set(xlabel='Epoch')\n",
    "    ax.legend(['train', 'val'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom R2-score metrics for keras backend\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('pollution.csv', header=0, index_col=0)\n",
    "data = data.dropna()\n",
    "values = data.values\n",
    "print('datapoints:', len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot showing the 5 years data for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [0, 1, 2, 3, 4, 5, 6]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize=(9,10))\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(data.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the index as date\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.resample('D').mean()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time series prediction, there is always the issue of how training and test data are to be split. For this purposes, we will define `TimeSeriesTrainTestSplit` where the last `test_size` data will be used as a test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeSeriesTrainTestSplit(X, test_size):\n",
    "    \n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "\n",
    "    X_train = X.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is a sequence prediction problem. Firstly, is needed to transform time series to a supervised learning problem. Given a sequence of values for time series data set, the data set can be structured to look like a supervised learning. For this purpose we are using a sliding window algorithm. We are using 90 previous days (time step) as input variables and we predict the next entry of polution data as output.\n",
    "\n",
    "![Moving Window Algorithm](sliding_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(data, timestamp):\n",
    "    \"\"\"\n",
    "    Split data into x (features) and y (target)\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    for i in range(timestamp, data.shape[0]):\n",
    "        x.append(data[i-timestamp:i,:])\n",
    "        y.append(data[i,-1:])\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of lag days\n",
    "timestamp =  90 # 1Q\n",
    "n_features = len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "df, test = TimeSeriesTrainTestSplit(data, 0.3)\n",
    "train, val = TimeSeriesTrainTestSplit(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_x_y(train.values, timestamp)\n",
    "X_val, y_val = get_x_y(val.values, timestamp)\n",
    "X_test, y_test = get_x_y(test.values, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scx = StandardScaler()\n",
    "scy = StandardScaler()\n",
    "\n",
    "X_train_sc = scx.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "y_train_sc = scy.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the training and validation data we have a tensor with _samples_ _x_ _timesteps_ _x_ _features_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = X_train.shape[1]\n",
    "n_feats = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_gru(n_steps,n_feats,n_fore=1):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, return_sequences=True, input_shape=(n_steps,n_feats),name=\"gru1\"))\n",
    "    model.add(GRU(128, name=\"gru2\"))\n",
    "    model.add(Dense(128,activation=\"relu\",name=\"hidden1\"))\n",
    "    model.add(Dense(64,activation=\"relu\",name=\"hidden2\"))\n",
    "    model.add(Dense(16,activation=\"relu\",name=\"hidden3\"))\n",
    "    model.add(Dense(n_fore,activation=\"linear\",name=\"output\"))\n",
    "    model.compile(loss='mse', optimizer='adam',metrics=[r2_keras])\n",
    "    return model\n",
    "\n",
    "def build_model_mlp(n_steps,n_feats,n_fore=1):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(n_steps,n_feats)))\n",
    "    model.add(Dense(64,activation=\"relu\",name=\"hidden1\"))\n",
    "    model.add(Dense(32,activation=\"relu\",name=\"hidden2\"))\n",
    "    model.add(Dense(16,activation=\"relu\",name=\"hidden3\"))\n",
    "    model.add(Dense(n_fore,activation=\"linear\",name=\"output\"))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=[r2_keras])\n",
    "    return model\n",
    "\n",
    "def build_model_cnn(n_steps,n_feats,n_fore=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=128, kernel_size=7, activation='relu',input_shape=(n_steps,n_feats)))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(n_fore, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[r2_keras])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_model_cnn(n_steps,n_feats)\n",
    "gru = build_model_gru(n_steps,n_feats)\n",
    "mlp = build_model_mlp(n_steps,n_feats)\n",
    "cb = TimeHistory()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_cnn = cnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[es, cb], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_mlp = mlp.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[es, cb], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_gru = gru.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[es, cb], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cb.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10,6))\n",
    "plot_model_history(history_cnn, ax = ax[0,0], plttitle='CNN')\n",
    "plot_model_history(history_mlp, ax = ax[0,1], plttitle='MLP')\n",
    "plot_model_history(history_gru, ax = ax[1,0], plttitle='GRU')\n",
    "#ax[0,0].set_ylim([300,2000])\n",
    "#ax[0,1].set_ylim([300,2000])\n",
    "#ax[1,0].set_ylim([300,2000])\n",
    "ax.flat[-1].set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [cnn,mlp,gru]\n",
    "modelnames = ['CNN', 'MLP', 'GRU']\n",
    "for i,nme in enumerate(modelnames):\n",
    "    print(nme)\n",
    "    print(models[i].evaluate(X_train,y_train))\n",
    "    print(models[i].evaluate(X_test,y_test))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_prediction(X):\n",
    "    y_pred = X[:,-1,-1]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = np.zeros((y_train.shape[0],4))\n",
    "y_pred_test = np.zeros((y_test.shape[0],4))\n",
    "modelnames = ['CNN', 'MLP', 'GRU', 'Dummy']\n",
    "for i,nme in enumerate(modelnames):\n",
    "    if i==3:\n",
    "        y_pred_train[:,i] = scy.inverse_transform(dummy_prediction(X_train)).ravel()\n",
    "        y_pred_test[:,i] = dummy_prediction(X_test).ravel()\n",
    "    else:\n",
    "        y_pred_train[:,i] = scy.inverse_transform(models[i].predict(X_train)).ravel()\n",
    "        y_pred_test[:,i] = models[i].predict(X_test).ravel()\n",
    "        \n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "for i,ax in enumerate(axs.flat):\n",
    "    textstr = 'RMSE training fit: %.03f\\n R2 training fit: %.03f\\n RMSE prediction: %.03f\\n R2 prediction: %.03f' % (np.sqrt(mean_squared_error(y_train,y_pred_train[:,i])),\n",
    "                                                                                                                    r2_score(y_train,y_pred_train[:,i]),\n",
    "                                                                                                                    np.sqrt(mean_squared_error(y_test,y_pred_test[:,i])),\n",
    "                                                                                                                    r2_score(y_test,y_pred_test[:,i]))\n",
    "    minlim = y_test.min()\n",
    "    maxlim = y_test.max()\n",
    "    sns.scatterplot(x=y_test.ravel(),y=y_pred_test[:,i],ax=ax)\n",
    "    ax.set_xlabel('observed pollution')\n",
    "    ax.set_ylabel('predicted pollution')\n",
    "    ax.set_xlim(minlim-10, maxlim+10)\n",
    "    ax.set_ylim(minlim-10, maxlim+10)\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    ax.set_title(modelnames[i])\n",
    "fig.suptitle('Test set predictions')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMAX\n",
    "\n",
    "Autoregressive Integrated Moving Average with Explanatory Variable (ARIMAX) is an extended version of ARIMA that includes independent predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date']=data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Almost every time series problem will have some external features or some internal feature engineering to help the model.\n",
    "\n",
    "Let's add some basic features like lag values of available numeric features that are widely used for time series problems. Since we need to predict the price of the stock for a day, we cannot use the feature values of the same day since they will be unavailable at actual inference time. We need to use statistics like mean, standard deviation of their lagged values.\n",
    "\n",
    "We will use three sets of lagged values, one looking back 7 days, one looking back a month (30 days) and another looking back 90 days as a proxy for last year quarter metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "lag_features = ['dew', 'temp', 'press', 'wnd_spd', 'snow', 'rain']\n",
    "\n",
    "window1 = 7 #weekly\n",
    "window2 = 30 #monthly\n",
    "window3 = 90 #Q\n",
    "\n",
    "df_rolled_7d = data[lag_features].rolling(window=window1, min_periods=0)\n",
    "df_rolled_30d = data[lag_features].rolling(window=window2, min_periods=0)\n",
    "df_rolled_90d = data[lag_features].rolling(window=window3, min_periods=0)\n",
    "\n",
    "df_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\n",
    "df_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n",
    "df_mean_90d = df_rolled_90d.mean().shift(1).reset_index().astype(np.float32)\n",
    "\n",
    "df_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\n",
    "df_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n",
    "df_std_90d = df_rolled_90d.std().shift(1).reset_index().astype(np.float32)\n",
    "\n",
    "for feature in lag_features:\n",
    "    data[f\"{feature}_mean_lag{window1}\"] = df_mean_7d[feature]\n",
    "    data[f\"{feature}_mean_lag{window2}\"] = df_mean_30d[feature]\n",
    "    data[f\"{feature}_mean_lag{window3}\"] = df_mean_90d[feature]\n",
    "    \n",
    "    data[f\"{feature}_std_lag{window1}\"] = df_std_7d[feature]\n",
    "    data[f\"{feature}_std_lag{window2}\"] = df_std_30d[feature]\n",
    "    data[f\"{feature}_std_lag{window3}\"] = df_std_90d[feature]\n",
    "\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "data.set_index(\"Date\", drop=False, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is very useful to add datetime features like hour, day, month, as applicable to provide the model information about the time component in the data. For time series models it is not explicitly required to pass this information but we could do so and we will try here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"month\"] = data.index.month\n",
    "data[\"week\"] = data.index.isocalendar().week\n",
    "data[\"day\"] = data.index.day\n",
    "data[\"day_of_week\"] = data.index.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and validation along with features. We will take the last year as a validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data[data.Date < \"2014\"]\n",
    "df_valid = data[data.Date >= \"2014\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Date'\n",
    "data.drop(['Date'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional features supplied to time series problems are called exogenous regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogenous_features = data.columns[data.columns != 'pollution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA (Auto Regressive Integrated Moving Average) models explain a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n",
    "\n",
    "ARIMA models require certain input parameters: p for the AR(p) part, q for the MA(q) part and d for the I(d) part. Thankfully, there is an automatic process by which these parameters can be chosen which is called Auto ARIMA.\n",
    "\n",
    "When exogenous regressors are used with ARIMA it is commonly called ARIMAX.\n",
    "\n",
    "Read more about [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(df_train['pollution'].values, exogenous=df_train[exogenous_features].values,\n",
    "                   trace=True, error_action='trace', suppress_warnings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df_train['pollution'].values, exogenous=df_train[exogenous_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_diagnostics(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "ax = sns.lineplot(x=df_train['Date'], y=df_train['pollution'], label='y_Train', color='b')\n",
    "ax = sns.lineplot(x=df_valid['Date'], y=df_valid['pollution'], label='y_Valid', color='g')\n",
    "\n",
    "ax = sns.lineplot(x=df_valid['Date'], y=forecast, label='y_pred', color='r')\n",
    "\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Pollution\")\n",
    "\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.lineplot(x=df_valid['Date'], y=df_valid['pollution'], label='y_Valid', color='g')\n",
    "sns.lineplot(x=df_valid['Date'], y=forecast, label=\"y_pred\", color='r')\n",
    "\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Pollution\")\n",
    "\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
