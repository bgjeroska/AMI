{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:Blue'> SEQUENCE CLASSIFICATION </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is based on the analysis of a dataset called “Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set” which is built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. Link to dataset: http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pmdarima as pm\n",
    "import statsmodels.api as sm\n",
    "import tsfresh\n",
    "import sklearn\n",
    "from scipy import signal \n",
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas import read_csv\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.patches import Rectangle\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['text.color'] = 'k'\n",
    "matplotlib.rcParams['figure.figsize'] = 13,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and plot one measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to know the dataset, we will start by visualising a single measurement from the first experiment done on user id 1. We highlight the actions that we would like to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    " \n",
    "exp01 = load_file('/Users/TNM1BET/Downloads/Dataset/acc_exp01_user01.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,8))\n",
    "plt.title('Triaxial acceleration signal (experiment id: 1 , user id: 1)',fontsize=20)\n",
    "#plt.xlabel('data points',fontsize=15)\n",
    "ax.plot(exp01[:,0], color= 'blue', label='acc X-axis')\n",
    "ax.plot(exp01[:,1], color= 'green', label='acc Y-axis')\n",
    "ax.plot(exp01[:,2], color= 'red', label='acc Z-axis')\n",
    "plt.ylabel('g (9.8 m/s^2)',fontsize=15)\n",
    "plt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "\n",
    "fig.gca().add_patch(Rectangle((250,-2),982,6,fill=True, color='g', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((1393,-2),801,6,fill=True, color='m', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((2360,-2),1014,6,fill=True, color='g', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((4736,-2),931,6,fill=True, color='m', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((7496,-2),582,6,fill=True, color='b', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((8356,-2),894,6,fill=True, color='b', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((9657,-2),910,6,fill=True, color='b', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((10750,-2),964,6,fill=True, color='b', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((13191,-2),655,6,fill=True, color='c', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((14069,-2),630,6,fill=True, color='r', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((14869,-2),623,6,fill=True, color='c', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((15712,-2),665,6,fill=True, color='r', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((16530,-2),623,6,fill=True, color='c', alpha=0.2, zorder=100, figure=fig))\n",
    "fig.gca().add_patch(Rectangle((17298,-2),672,6,fill=True, color='r', alpha=0.2, zorder=100, figure=fig))\n",
    "\n",
    "plt.figtext(0.25, 0.06, \"Standing\", ha=\"center\", fontsize=18, bbox={\"facecolor\":\"green\", \"alpha\":0.2, \"pad\":5})\n",
    "plt.figtext(0.35, 0.06, \"Sitting\", ha=\"center\", fontsize=18, bbox={\"facecolor\":\"magenta\", \"alpha\":0.2, \"pad\":5})\n",
    "plt.figtext(0.45, 0.06, \"Walking\", ha=\"center\", fontsize=18, bbox={\"facecolor\":\"blue\", \"alpha\":0.2, \"pad\":5})\n",
    "plt.figtext(0.75, 0.06, \"Walking_upstairs\", ha=\"center\", fontsize=18, bbox={\"facecolor\":\"red\", \"alpha\":0.2, \"pad\":5})\n",
    "plt.figtext(0.60, 0.06, \"Walking_downstairs\", ha=\"center\", fontsize=18, bbox={\"facecolor\":\"cyan\", \"alpha\":0.2, \"pad\":5})\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go now deeper and for that we will pick one segment from the above measurement which represents the walking action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_walk = exp01[10750:11714,:]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(segment_walk[:,0], color= \"blue\", label=\"X-axis\")\n",
    "plt.plot(segment_walk[:,1], color= \"green\", label=\"Y-axis\")\n",
    "plt.plot(segment_walk[:,2], color=\"red\", label=\"Z-axis\")\n",
    "plt.title('One segment from the walking action',fontsize=15)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('datapoints',fontsize=15)\n",
    "plt.ylabel('g (9.8 m/s^2)',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and major step in time series classification is the pre-processing step. We will now go through the different steps using the snippet from walking activity. \n",
    "We will:\n",
    "1. downsample the data to smooth the signal\n",
    "2. roll a sliding window on the signal and cut the signal in same sized windows  \n",
    "3. normalize and bounde each window within [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_walk_down = signal.decimate(segment_walk, q=2, n=0,ftype=\"fir\", axis=0)\n",
    "print('Downsampled with Factor: {}, filter: type: {}, of order {}'.format(2,\"fir\",0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_2d(arr, window_size, shifting_distance, slicing_axis=0, window_axis=1):\n",
    "    \n",
    "        array = np.expand_dims(arr, window_axis) # new shape: n x 1 x 3\n",
    "        mod_shape = list(array.shape)\n",
    "        #print('array shape:', mod_shape)\n",
    "        mod_shape[slicing_axis] = np.floor(array.shape[slicing_axis] / shifting_distance \n",
    "                                        - window_size / shifting_distance + 1).astype(int)\n",
    "        mod_shape[window_axis] = window_size\n",
    "        mod_strides = list(array.strides)\n",
    "        mod_strides[slicing_axis] *= shifting_distance # shift this far in original array for steps on 0-axis in new array\n",
    "        mod_strides[window_axis] = array.strides[slicing_axis] \n",
    "        row_window_array = np.lib.stride_tricks.as_strided(array, shape=mod_shape, strides=mod_strides)\n",
    "            \n",
    "        print(\"number of windows:\",row_window_array.shape[0])\n",
    "        print(\"number of observation per window:\",row_window_array.shape[1])\n",
    "        print(\"number of axis:\",row_window_array.shape[2])\n",
    "    \n",
    "        return row_window_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128\n",
    "step_size = 64   #if equal to window_size than no overlapping\n",
    "\n",
    "X_windowed = sliding_window_2d(segment_walk_down, window_size, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_vibration(data_array):\n",
    "    X = np.ndarray(shape=(data_array.shape))\n",
    "    for window in range(len(data_array)):\n",
    "        X[window,:,:] = sklearn.preprocessing.maxabs_scale(data_array[window,:,:])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scl= scale_data_vibration(X_windowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the results from the pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2,2,figsize=(12,8), sharey=False)\n",
    "fig.suptitle(f'Window snipping with length{window_size} and step size of {step_size}')\n",
    "ax1.plot(X_windowed[0])\n",
    "ax1.set_title(\"window number: 1\")\n",
    "ax2.plot(X_windowed[1])\n",
    "ax2.set_title(\"window number: 2\")\n",
    "ax3.plot(X_scl[0])\n",
    "ax3.set_title(\"scaled window number: 1\")\n",
    "ax4.plot(X_scl[1])\n",
    "ax4.set_title(\"scaled window number: 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have standardized, same sized windows we can start by processing those. Next step would be the feature extraction. For that we can use tsfresh which allows us to derive a comprehensive set of characteristics from the rawdata. To get more insight here the link: https://tsfresh.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need of course to start by reshaping our data in the correct format required for the tsfresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "n_channels = X_scl.shape[2]\n",
    "n_obs = X_scl.shape[1]\n",
    "windows = X_scl.shape[0]\n",
    "df = pd.DataFrame()\n",
    "for window in range(windows):\n",
    "    df_temp = pd.DataFrame()\n",
    "    for channel in range(n_channels):\n",
    "        df_temp['acc_' + str(channel)] = pd.Series(X_scl[window,:,channel])\n",
    "    df_temp.insert(0, 'id', idx)\n",
    "    df_temp.insert(1, 'time', range(n_obs))\n",
    "    idx+=1\n",
    "    if df.empty:\n",
    "        df = df_temp\n",
    "    else:\n",
    "        df = pd.concat([df,df_temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of simplicity and time, we will calculate only the minimal number of features. Using the EfficientFCParameters would allow you to extract a higher number of features. \n",
    "In this link you can have an overview to the features list which can be calculated: https://tsfresh.readthedocs.io/en/latest/text/list_of_features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "extraction_settings = MinimalFCParameters()\n",
    "#extraction_settings = EfficientFCParameters()\n",
    "\n",
    "X_features = extract_features(df_copy, column_id='id', column_sort='time',\n",
    "         default_fc_parameters=extraction_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features extracted:\",X_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2,2,figsize=(12,8))\n",
    "fig.suptitle('Some Features extracted')\n",
    "ax1.plot(X_features[\"acc_0__sum_values\"])\n",
    "ax2.plot(X_features[\"acc_0__standard_deviation\"])\n",
    "ax3.plot(X_features[\"acc_0__mean\"])\n",
    "ax4.plot(X_features[\"acc_0__variance\"])\n",
    "ax1.set_title('acc_0__sum_values')\n",
    "ax2.set_title('acc_0__standard_deviation')\n",
    "ax3.set_title('acc_0__mean')\n",
    "ax4.set_title('acc_0__variance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the ML Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load now the train and test sets. Each set contains:\n",
    "- A 561-feature vector with time and frequency domain variables. \n",
    "- Its associated activity label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_file(\"/Users/TNM1BET/Downloads/Dataset/X_train_reduced.txt\")\n",
    "y_train = load_file(\"/Users/TNM1BET/Downloads/Dataset/y_train_reduced.txt\")\n",
    "X_test = load_file(\"/Users/TNM1BET/Downloads/Dataset/X_test_reduced.txt\")\n",
    "y_test = load_file(\"/Users/TNM1BET/Downloads/Dataset/y_test_reduced.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_analysis(data, bins):\n",
    "    df = pd.DataFrame(data)\n",
    "    elements={}\n",
    "    counts = df.groupby(0).size()\n",
    "    counts = counts.values\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"]\n",
    "np_samples_train = class_balance_analysis(y_train,bins)\n",
    "np_samples_test = class_balance_analysis(y_test,bins)\n",
    "\n",
    "fig, ((ax1, ax2)) = plt.subplots(1,2,figsize=(12,3))\n",
    "ax1.bar(bins,np_samples_train)\n",
    "ax2.bar(bins,np_samples_test)\n",
    "ax1.set_title('samples balance training set')\n",
    "ax2.set_title('samples balance test set')\n",
    "ax1.xaxis.set_tick_params(rotation=50)\n",
    "ax2.xaxis.set_tick_params(rotation=50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 70,\n",
    "    'learning_rate' : 0.5,\n",
    "}\n",
    "\n",
    "ada = AdaBoostClassifier(**ada_params)\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = ada.predict(X_train)\n",
    "y_pred_test = ada.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'Mean accuracy train score: {acc_train:.3}')\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Mean accuracy test score: {acc_test:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_train = confusion_matrix(y_train,y_pred_train)\n",
    "conf_test = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "conf_train_norm = conf_train.astype('float') / conf_train.sum(axis=1)[:, np.newaxis]\n",
    "conf_test_norm = conf_test.astype('float') / conf_test.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fg, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2,2,figsize=(12,10))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "#ax1.set_xticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax1.set_yticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "#ax2.set_xticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax2.set_yticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax2.set(title=\"Confusion Matrix for test set\")\n",
    "sns.heatmap(conf_train_norm, annot=True, ax=ax3)\n",
    "ax3.set(xlabel=\"predicted label\")\n",
    "#ax3.set_xticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax3.set_yticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax3.set(ylabel=\"actual label\")\n",
    "ax3.set(title=\"Normalized Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test_norm, annot=True, ax=ax4)\n",
    "ax4.set(xlabel=\"predicted label\")\n",
    "ax4.set(ylabel=\"actual label\")\n",
    "#ax4.set_xticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax4.set_yticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax4.set(title=\"Normalized Confusion Matrix for test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost parameters\n",
    "grad_params = {\n",
    "    'n_estimators': 40,\n",
    "    'learning_rate' : 0.5,\n",
    "    'max_depth' : 1,\n",
    "    'random_state' : 0\n",
    "}\n",
    "\n",
    "grad = GradientBoostingClassifier(**grad_params)\n",
    "\n",
    "grad.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = grad.predict(X_train)\n",
    "y_pred_test = grad.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'Mean accuracy train score: {acc_train:.3}')\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Mean accuracy test score: {acc_test:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_train = confusion_matrix(y_train,y_pred_train)\n",
    "conf_test = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "conf_train_norm = conf_train.astype('float') / conf_train.sum(axis=1)[:, np.newaxis]\n",
    "conf_test_norm = conf_test.astype('float') / conf_test.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fg, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2,2,figsize=(12,10))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "#ax1.set_xticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax1.set_yticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "#ax2.set_xticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax2.set_yticklabels([\"Walking\",\"W_upstairs\",\"W_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax2.set(title=\"Confusion Matrix for test set\")\n",
    "sns.heatmap(conf_train_norm, annot=True, ax=ax3)\n",
    "ax3.set(xlabel=\"predicted label\")\n",
    "#ax3.set_xticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax3.set_yticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax3.set(ylabel=\"actual label\")\n",
    "ax3.set(title=\"Normalized Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test_norm, annot=True, ax=ax4)\n",
    "ax4.set(xlabel=\"predicted label\")\n",
    "ax4.set(ylabel=\"actual label\")\n",
    "#ax4.set_xticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "#ax4.set_yticklabels([\"Walking\",\"Walking_upstairs\",\"Walking_downstairs\",\"Sitting\",\"Standing\"])\n",
    "ax4.set(title=\"Normalized Confusion Matrix for test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
