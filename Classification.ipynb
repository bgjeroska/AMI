{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:Blue'> Classification  </span>\n",
    "\n",
    "In this notebook, we will be working with the [Fetal Health Dataset](https://www.kaggle.com/andrewmvd/fetal-health-classification).\n",
    "This dataset contains 2126 records of features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into 3 classes:\n",
    "\n",
    "* `Normal`\n",
    "\n",
    "* `Suspect`\n",
    "\n",
    "* `Pathological`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "To develop our classification model, we need to import the necessary Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data\n",
    "\n",
    "Load and show the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into the source variables (independant variables) and the target variable (dependant variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make the model, we need to split the data into train dataset and test dataset. We will use the train dataset to train the classification model. The test dataset will be used as a comparasion and see the performance of our model. We will 67% of the data as the training data and the rest of it as the testing data. Also we are using stratified Train-Test split, that is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "sns.histplot(data.iloc[:,-1],kde=False,label='All', ax=ax)\n",
    "sns.histplot(y_train+.05, kde=False, label='train', color='green', ax=ax)\n",
    "sns.histplot(y_test+.1, kde=False, label='test', color='orange', ax=ax)\n",
    "plt.xlabel('fetal_health')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks([1, 2, 3],['Normal','Suspect','Pathological'])\n",
    "plt.title(\"Distribution of Classes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#ros = RandomOverSampler(random_state=0)\n",
    "#X_data, y_data = ros.fit_resample(data.iloc[:,:-1], data.iloc[:,-1])\n",
    "#X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f, ax = plt.subplots()\n",
    "#sns.histplot(y_data,kde=False,label='All', ax=ax)\n",
    "#sns.histplot(y_train+.05, kde=False, label='train', color='green', ax=ax)\n",
    "#sns.histplot(y_test+.1, kde=False, label='test', color='orange', ax=ax)\n",
    "#plt.xlabel('fetal_health')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.xticks([1, 2, 3],['Normal','Suspect','Pathological'])\n",
    "#plt.title(\"Distribution of Classes\")\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random Forest Image](randomforest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest consists of multiple decision trees, which predict the label given the input features. The overall output is then averaged over the predictions of the multiple trees in the forest. This prediction scheme helps to avoid overfitting, since the forest is an \"ensemble\" of a large number of trees. Each tree is given a subset of the samples and / or features in order to create an \"expert tree\" that is fit to a subportion of the dataset. \n",
    "\n",
    "We now define a random forest with a pre-defined number of trees (\"n_estimators\") and maximum number of estimators (\"max_features\"). More details about what the parameters mean are given in [what these parameters mean](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "rfc = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 42, max_depth=2)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rfc.predict(X_train)\n",
    "y_pred_test = rfc.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'Mean accuracy train score: {acc_train:.3}')\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Mean accuracy test score: {acc_test:.3}')\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_train = confusion_matrix(y_train,y_pred_train)\n",
    "conf_test = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "ax1.set_xticklabels(['Normal','Suspect','Pathological'])\n",
    "ax1.set_yticklabels(['Normal','Suspect','Pathological'])\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "ax2.set_xticklabels(['Normal','Suspect','Pathological'])\n",
    "ax2.set_yticklabels(['Normal','Suspect','Pathological'])\n",
    "ax2.set(title=\"Confusion Matrix for test set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access each single tree on our forest as shown below. You can see that the trees are not pruned (ccp_alhpa=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.estimators_[98:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cocalc Task**: Perform a grid search to come up with better parameters for our dataset than the pre-defined ones. \n",
    "\n",
    "Bonus: Are there other parameters of the tree, which you can change to improve on the results on our data set? Which parameters are most / least influence on the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsN = [50, 100, 500, 1000]\n",
    "gsK = [2, \"auto\", \"log2\"]\n",
    "\n",
    "\n",
    "### Validation\n",
    "cval = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid = {'n_estimators': gsN,\n",
    "              'max_features': gsK}\n",
    "\n",
    "search = GridSearchCV(model, param_grid, n_jobs=-1,cv=cval,return_train_score=True)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Accuracy=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Check the result for our test set\n",
    "best_estimator = search.best_estimator_\n",
    "y_pred_train = best_estimator.predict(X_train)\n",
    "y_pred_test = best_estimator.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'Mean accuracy train score: {acc_train:.3}')\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Mean accuracy test score: {acc_test:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_train = confusion_matrix(y_train,y_pred_train)\n",
    "conf_test = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "ax1.set_xticklabels(['Normal','Suspect','Pathological'])\n",
    "ax1.set_yticklabels(['Normal','Suspect','Pathological'])\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "ax2.set_xticklabels(['Normal','Suspect','Pathological'])\n",
    "ax2.set_yticklabels(['Normal','Suspect','Pathological'])\n",
    "ax2.set(title=\"Confusion Matrix for test set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting \"free candy\" of Trees is the fact that you can rank importantance of each feature. This is given by the `estimator.feature_importances_` attribute in sklearn, which gives you an impurity-based ranking of the features. If you like to have more statistics on the importance, you can retrieve the importances of all trees in the forest and then calculate the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the forest\n",
    "\n",
    "rfc = search.best_estimator_\n",
    "\n",
    "# Visualize the feature importances\n",
    "importances = rfc.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, data.drop('fetal_health',axis=1).columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances for Random Forest Classification\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), data.drop('fetal_health',axis=1).columns[indices],rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Extract single tree\n",
    "estimator = rfc.estimators_[2]\n",
    "\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = X.columns,\n",
    "                class_names = 'fetal_health',\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "\n",
    "# Display in jupyter notebook\n",
    "#from IPython.display import Image\n",
    "#Image(filename = 'tree.png')\n",
    "\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure()\n",
    "plot_tree(rfc.estimators_[2], filled=True, feature_names=X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the stability of the features\n",
    "As always, our fitted model will depend on the samples that we used for training. Ideally, the ranking of the features will not be affected, but it's always recommendable to have a look at it. In real world datasets it can happen that your features are not stable and change their ranking with varying training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Cocalc Task:** How stable is the feature importance when we use different train/test splits? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 1000\n",
    "max_features = 3\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "f, axs = plt.subplots(1,3,figsize=(12,3))\n",
    "\n",
    "for num, [train_index, test_index] in enumerate(kf.split(X)):\n",
    "    \n",
    "    # split into training and testing data\n",
    "    X_train = X.iloc[train_index, :]\n",
    "    y_train = Y[train_index]\n",
    "    \n",
    "    # not needed but we still do it for completeness\n",
    "    X_test = X.iloc[test_index, :]\n",
    "    y_test = Y[test_index]\n",
    "    \n",
    "    print(\"For fold: %s\" %(num))\n",
    "    \n",
    "    # Fit the forest\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, n_jobs=-1)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    # Visualize the feature importances\n",
    "    importances = rfc.feature_importances_\n",
    "    \n",
    "    std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    #for f in range(X.shape[1]):\n",
    "    #    print(\"%d. %s (%f)\" % (f + 1, data.drop('fetal_health',axis=1).columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "    # Plot the impurity-based feature importances of the forest\n",
    "    #plt.figure()\n",
    "    \n",
    "    axs[num].set_title(\"Feature importances for Random\\nForest Classification - Fold %s \" %(num))\n",
    "    axs[num].bar(range(X.shape[1]), importances[indices],\n",
    "            color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    axs[num].set_xticks(range(X.shape[1]))\n",
    "    axs[num].set_xticklabels( data.drop('fetal_health',axis=1).columns[indices],rotation=90)\n",
    "    axs[num].set_xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Adaptive Boosting (AdaBoost)\n",
    "\n",
    "In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement. In contrast to the original boosting procedure, AdaBoost uses the complete training dataset to train the weak learners, where the training examples are reweighted in each iteration to build a strong classifier that learns from the mistakes of the previous weak learners in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.1,\n",
    "    'random_state': 1\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              random_state=1,\n",
    "                              max_depth=1)\n",
    "\n",
    "ada = AdaBoostClassifier(base_estimator = tree,\n",
    "                         **ada_params)\n",
    "\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = tree.predict(X_train)\n",
    "y_pred_test = tree.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'Mean accuracy train score: {acc_train:.3}')\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Mean accuracy test score: {acc_test:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = ada.predict(X_train)\n",
    "y_pred_test = ada.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'Mean accuracy train score: {acc_train:.3}')\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'Mean accuracy test score: {acc_test:.3}')\n",
    "\n",
    "\n",
    "fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set(xlabel=\"predicted label\")\n",
    "ax1.set_xticklabels(['Normal','Suspect','Pathological'])\n",
    "ax1.set_yticklabels(['Normal','Suspect','Pathological'])\n",
    "ax1.set(ylabel=\"actual label\")\n",
    "ax1.set(title=\"Confusion Matrix for training set\")\n",
    "sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set(xlabel=\"predicted label\")\n",
    "ax2.set(ylabel=\"actual label\")\n",
    "ax2.set_xticklabels(['Normal','Suspect','Pathological'])\n",
    "ax2.set_yticklabels(['Normal','Suspect','Pathological'])\n",
    "ax2.set(title=\"Confusion Matrix for test set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the AdaBoost model predicts all class labels of the training dataset correctly and also shows a slightly improved test dataset performance compared to the decision tree stump. However, you can also see that we introduced additional variance by our attempt to reduce the model bias—a greater gap between training and test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature importances\n",
    "importances = ada.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in ada.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, data.drop('fetal_health',axis=1).columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances for AdaBoost Classifier\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), data.drop('fetal_health',axis=1).columns[indices],rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, [train_index, test_index] in enumerate(kf.split(X)):\n",
    "    \n",
    "    # split into training and testing data\n",
    "    X_train = X.iloc[train_index,:]\n",
    "    y_train = Y[train_index]\n",
    "    \n",
    "    # not needed but we still do it for completeness\n",
    "    X_test = X.iloc[test_index,:]\n",
    "    y_test = Y[test_index]\n",
    "    \n",
    "    print(\"For fold: %s\" %(num))\n",
    "    \n",
    "    # Fit the forest\n",
    "    ada = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "    ada.fit(X_train, y_train)\n",
    "    \n",
    "    # Visualize the feature importances\n",
    "    importances = ada.feature_importances_\n",
    "    \n",
    "    std = np.std([tree.feature_importances_ for tree in ada.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    #for f in range(X.shape[1]):\n",
    "    #    print(\"%d. %s (%f)\" % (f + 1, data.drop('csMPa',axis=1).columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "    # Plot the impurity-based feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances for AdaBoost Classifier - Fold %s \" %(num))\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "            color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), data.drop('fetal_health',axis=1).columns[indices],rotation=90)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf_labels = ['Random Forest', 'Decision Tree', 'AdaBoost']\n",
    "\n",
    "all_clf = [rfc, tree, ada]\n",
    "\n",
    "\n",
    "for clf, la in zip(all_clf, clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv = cval,\n",
    "                             scoring='roc_auc_ovo')\n",
    "    print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\"\n",
    "    % (scores.mean(), scores.std(), la))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
